{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from nltk import Tree\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.optimizer import required, Optimizer\n",
    "from torch.autograd import grad\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class represeting the one-hot encoder of the states.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Encoder.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        d          : Int, the input dimension.\n",
    "        dim        : Int, the one-hot encoded representation dimension\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.categories = categories\n",
    "        self.d = len(categories)\n",
    "        self.dim = np.sum(categories) + self.d\n",
    "        \n",
    "    def transform(self, state_values):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the vector state representation with dummies.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        state_values : List of length d where the ith entry is either NaN or the the feature value.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        state_one_hot : 2D np.array representing the one-hot encoded state.\n",
    "        \"\"\"\n",
    "    \n",
    "        s = 0\n",
    "        state_one_hot = np.zeros((1, self.dim), dtype = np.float32)\n",
    "        for i, value in enumerate(state_values):\n",
    "            if np.isnan(value):\n",
    "                state_one_hot[0, self.categories[i] + s] = 1\n",
    "\n",
    "            else:\n",
    "                state_one_hot[0, value + s] = 1\n",
    "\n",
    "            s += self.categories[i]+1\n",
    "\n",
    "        return state_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateDQN:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing a state, it also serves as Node representation\n",
    "    for our Breadth First Search\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, values, encoder, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class State.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        values     : List of length d (Input dimension):\n",
    "                         - values[i] = NaN if i is an unobsorved feature.\n",
    "                         - values[i] = value of feature i if it is observed.\n",
    "        encocer    : sklearn.preprocessing._encoders.OneHotEncoder object.\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        observed   : List containing the observed features at this state.\n",
    "        unobserved : List containing the unobserved features at this state.\n",
    "        empty      : Boolean, whether it is the empty state or not.\n",
    "        complete   : Boolean, whether all features are observed or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        d = len(values)\n",
    "        values_nans = np.isnan(values)\n",
    "        self.encoder = encoder  # One-hot encoder, used in the approximate RL framework for state representation.\n",
    "        self.values = values\n",
    "        self.values_encoded = self.encode()\n",
    "        self.categories = categories\n",
    "        self.observed = np.arange(d)[np.invert(values_nans)]\n",
    "        self.unobserved = np.arange(d)[values_nans]  # These are also the allowed query actions at this state\n",
    "        self.empty = (len(self.observed) == 0)\n",
    "        self.complete = (len(self.unobserved) == 0)\n",
    "        \n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the state with dummy variables. To be used when a one-hot encoder is defined.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        np.array of shape (1, #one_hot_representation_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.encoder.transform(self.values)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=25, out=8):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        Constructor of ActorCritic class.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        input_size    : Int, the one-hot encoding representation dimension.\n",
    "        out           : Int, Actor output dimension, equal to the number of possible actions.\n",
    "        fc_1          : nn.Linear, first fully connected layer (common parameters between Actor and Critic).\n",
    "        fc_2          : nn.Linear, second fully connected layer (common parameters between Actor and Critic).\n",
    "        actor_output  : nn.Linear, actor output fully connected layer.\n",
    "        critic_output : nn.Linear, critic output fully connected layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.out = out\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_size, 32)\n",
    "        self.fc_2 = nn.Linear(32, 32)\n",
    "        \n",
    "        self.actor_output = nn.Linear(32, out)\n",
    "        \n",
    "        nn.init.constant_(self.actor_output.weight, 0)\n",
    "        nn.init.constant_(self.actor_output.bias, 0)\n",
    "                                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        The forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        x : torch.tensor of dimension (batch_size, input_size)\n",
    "        \n",
    "        Returns\n",
    "        ---------------\n",
    "        value_output  : torch.tensor of dimension (batch_size, 1)\n",
    "        policy_output : torch.tensor of dimension (batch_size, out)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        \n",
    "        return self.actor_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=25):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        Constructor of ActorCritic class.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        input_size    : Int, the one-hot encoding representation dimension.\n",
    "        out           : Int, Actor output dimension, equal to the number of possible actions.\n",
    "        fc_1          : nn.Linear, first fully connected layer (common parameters between Actor and Critic).\n",
    "        fc_2          : nn.Linear, second fully connected layer (common parameters between Actor and Critic).\n",
    "        actor_output  : nn.Linear, actor output fully connected layer.\n",
    "        critic_output : nn.Linear, critic output fully connected layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_size, 32)\n",
    "        self.fc_2 = nn.Linear(32, 32)\n",
    "        \n",
    "        self.critic_output = nn.Linear(32, 1)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        The forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        x : torch.tensor of dimension (batch_size, input_size)\n",
    "        \n",
    "        Returns\n",
    "        ---------------\n",
    "        value_output  : torch.tensor of dimension (batch_size, 1)\n",
    "        policy_output : torch.tensor of dimension (batch_size, out)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        \n",
    "        return self.critic_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentDQN:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing the environment, it can generate data points that start each episode,\n",
    "    keep track of the current state, return the reward of an action taken at the current state,\n",
    "    and transition to the next corresponding state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, generator, rewards_queries, encoder, r_plus=5, r_minus=-5, split=3):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Environment.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        generator       : Dict, - keys   : Feature variables.\n",
    "                                - values : List of probability masses of each category of the corresponding feature.\n",
    "        rewards_queries : Dict, - keys   : Feature variables.\n",
    "                                - values : Reward of querying the value of the corresponding feature.\n",
    "        encoder         : Object of class Encoder, the encoder mapping states to their one-hot representation.\n",
    "        r_plus          : Int, reward of a correct report (default=5).\n",
    "        r_minus         : Int, reward of an incorrect report (default=-5).\n",
    "        split           : Int, the split point we use to define our concept.\n",
    "        d               : Int, the number of feature variables.\n",
    "        data_point      : List of length d, the data point starting the episode.\n",
    "        label           : Boolean, the true label of data_point.\n",
    "        state           : Object of class State, the current state.\n",
    "        done            : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.generator = generator\n",
    "        self.categories = [len(v) for v in self.generator.values()]\n",
    "        self.d = len(self.categories)\n",
    "        self.rewards_queries = rewards_queries\n",
    "        self.encoder = encoder\n",
    "        self.r_plus = r_plus\n",
    "        self.r_minus = r_minus\n",
    "        self.split = split\n",
    "        \n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Generate a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        List with the values of each feature, it represents the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        return [np.random.choice(self.categories[i], p=self.generator[i]) for i in range(self.d)]\n",
    "    \n",
    "    def concept(self, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Define the concept labeling the data points. we can define it as a decision tree for example.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        Int in {0, 1}, the label of the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        label = True\n",
    "        i = 0\n",
    "        while label and i <= self.d-1:\n",
    "            if data_point[i] >= self.split:\n",
    "                label = False\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def reset(self, data_point=None):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Reset the environment to start a new episode. If data_point is specified, start the episode from it,\n",
    "        otherwise generate it.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label (default=None).\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_point = self.generate() if data_point is None else data_point\n",
    "        self.label = self.concept(self.data_point)\n",
    "        self.state = StateDQN([np.NaN for i in range(self.d)], self.encoder, categories=self.categories)\n",
    "        self.done = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Interract with the environment through an action taken at the current state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}, \n",
    "                 - 0, ..., d-1 represent query actions.\n",
    "                 - d, d+1 represent report actions.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        reward     : Int, the reward of taking this action at the current state.\n",
    "        next_state : Object of class State, the next state.\n",
    "        done       : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Treating query actions.\n",
    "        if action <= self.d-1:\n",
    "            # If it is an allowed query action.\n",
    "            if np.isnan(self.state.values[action]):\n",
    "                reward = self.rewards_queries[action]\n",
    "                values = self.state.values\n",
    "                values[action] = self.data_point[action] # Reveal the value of the queried feature in the data point.\n",
    "                self.state = StateDQN(values, self.encoder, self.categories)\n",
    "                \n",
    "            # If this query action is not allowed.\n",
    "            else:\n",
    "                print('unallowed')\n",
    "            \n",
    "        # Treating report actions.\n",
    "        else:\n",
    "            reward = self.r_plus if (action%self.d) == self.label else self.r_minus\n",
    "            self.done = True\n",
    "            \n",
    "        return reward, self.state, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_Js(probs, params):\n",
    "    grad_Js = []\n",
    "    ts = []\n",
    "    pi_s = []\n",
    "    for i, prob in enumerate(probs):\n",
    "        pi = prob.flatten()\n",
    "        t = torch.ones_like(pi, requires_grad=True)\n",
    "        grad_J = parameters_to_vector(grad(torch.dot(pi, t), params, create_graph=True))\n",
    "        \n",
    "        pi_s.append(pi)\n",
    "        ts.append(t)\n",
    "        grad_Js.append(grad_J)\n",
    "\n",
    "    return grad_Js, ts, pi_s\n",
    "\n",
    "\n",
    "# def hessian_vector_product(x, probs, params):\n",
    "    \n",
    "#     b_s = []\n",
    "#     for i, prob in enumerate(probs):\n",
    "#         pi = prob.flatten()\n",
    "#         t = torch.ones_like(pi, requires_grad=True)\n",
    "#         grad_J = parameters_to_vector(grad(torch.dot(pi, t), params, create_graph=True))\n",
    "#         a = grad(torch.dot(grad_J, x), t, retain_graph=True)[0]\n",
    "#         b = (a/pi).detach()\n",
    "#         b_s.append(b)\n",
    "        \n",
    "#     return parameters_to_vector(grad(probs, params, grad_outputs=b_s, retain_graph=True))/len(probs)\n",
    "\n",
    "def hessian_vector_product(x, probs, params, grad_Js, ts, pi_s):\n",
    "    \n",
    "    b_s = []\n",
    "    for i, prob in enumerate(probs):\n",
    "        a = grad(torch.dot(grad_Js[i], x), ts[i], retain_graph=True)[0]\n",
    "        b = (a/pi_s[i]).detach()\n",
    "        b_s.append(b)\n",
    "        \n",
    "    return parameters_to_vector(grad(probs, params, grad_outputs=b_s, retain_graph=True))/len(probs)    \n",
    "\n",
    "def conjugate_gradient(H, b, x_0, tol=1e-3, max_iter=100):\n",
    "    \n",
    "    x_t = x_0\n",
    "    r_t = (b - H(x_t)).detach()\n",
    "    p_t = r_t.clone()\n",
    "    it = 0\n",
    "    while (torch.norm(r_t) > tol) and (it < max_iter):\n",
    "        H_p_t = H(p_t).detach()\n",
    "        prod_r_t = torch.dot(r_t.flatten(), r_t.flatten())\n",
    "        prod_p_t = torch.dot(p_t.flatten(), H_p_t.flatten())\n",
    "        alpha = prod_r_t/prod_p_t\n",
    "        x_t = x_t + alpha*p_t\n",
    "        r_t = r_t - alpha*H_p_t\n",
    "        prod_r_t1 = torch.dot(r_t.flatten(), r_t.flatten())\n",
    "        beta = prod_r_t1/prod_r_t\n",
    "        p_t = r_t + beta*p_t\n",
    "        it += 1\n",
    "                \n",
    "    return x_t, (torch.norm(r_t) <= tol), it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaturalGradient(Optimizer):\n",
    "    \n",
    "    def __init__(self, params, input_size, out, min_queries, d, b, max_j=100, coef=0.1, delta=1e-2, tol=1e-3):\n",
    "        \n",
    "        defaults = dict(coef=coef, delta=delta, tol=tol, grads=[], grads_nat=[], step=0, input_size=input_size, out=out, \n",
    "                        min_queries=min_queries, d=d, b=b, max_j=max_j)\n",
    "        super(NaturalGradient, self).__init__(params, defaults)\n",
    "                \n",
    "    def compute_grads(self, probs):\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            group['grads']=[]\n",
    "            group['grads_nat']=[]\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                group['grads'].append(p.grad)\n",
    "                group['grads_nat'].append(torch.empty_like(p))\n",
    "                \n",
    "            grads_vector = parameters_to_vector(group['grads'])\n",
    "            grad_Js, ts, pi_s = compute_grad_Js(probs, group['params'])\n",
    "#             H = lambda x : hessian_vector_product(x, probs, group['params'], grad_Js, ts, pi_s)\n",
    "            H = lambda x : hessian_vector_product(x, probs, group['params'], grad_Js, ts, pi_s)\n",
    "            grad_nat_vector, conv, it = conjugate_gradient(H, grads_vector, x_0=torch.zeros_like(grads_vector), max_iter=len(grads_vector), tol=group['tol'])\n",
    "            step = np.sqrt(2*group['delta']/(torch.dot(grad_nat_vector, H(grad_nat_vector)).item()))\n",
    "            vector_to_parameters(grad_nat_vector, group['grads_nat'])\n",
    "            group['step'] = step\n",
    "                        \n",
    "    @torch.no_grad()\n",
    "    \n",
    "    def compute_KL(self, probs, states):\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            actor_new = Actor(group['input_size'], group['out'])\n",
    "            params_new = list(actor_new.parameters())\n",
    "            i = 0\n",
    "            for p_new, p in zip(actor_new.parameters(), group['params']):\n",
    "                p_new.zero_()\n",
    "                p_new.add_(p)\n",
    "                if p.grad is not None:\n",
    "                    p_new.add_(group['grads_nat'][i], alpha=-group['step'])\n",
    "        \n",
    "                i += 1\n",
    "        \n",
    "        kl = 0\n",
    "        for i, state in enumerate(states):\n",
    "            actions_output = actor_new(torch.from_numpy(state.values_encoded))\n",
    "            if len(state.observed) < group['min_queries']:\n",
    "                actions_allowed = list(state.unobserved)\n",
    "\n",
    "            else:\n",
    "                actions_allowed = list(state.unobserved) + [group['d']+i for i in range(group['b'])]\n",
    "\n",
    "            actions_probas = F.softmax(actions_output[0, actions_allowed], dim=0)\n",
    "            kl += (probs[i]*torch.log(probs[i]/actions_probas)).sum()\n",
    "            \n",
    "        return kl/len(states)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    \n",
    "    def line_search(self, probs, states):\n",
    "        \n",
    "        j = 0\n",
    "        kl = self.compute_KL(probs, states)\n",
    "        for group in self.param_groups:\n",
    "            while (kl > group['delta']) and (j <= group['max_j']):\n",
    "                group['step'] *= group['coef']\n",
    "                kl = self.compute_KL(probs, states)\n",
    "                j += 1\n",
    "                \n",
    "        return kl\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    \n",
    "    def step(self, probs, states, closure=None):\n",
    "        \n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "                \n",
    "        kl = self.line_search(probs, states)\n",
    "        for group in self.param_groups:\n",
    "            for i, p in enumerate(group['params']):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                p.add_(group['grads_nat'][i], alpha=-group['step'])\n",
    "                \n",
    "#         return loss\n",
    "        return loss, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTRPO:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class describing a DQN agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=.9, categories=[4, 3, 3, 3, 2, 4], labels=[0, 1], min_queries=4):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class AgentDQN.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        gamma           : Float in ]0, 1[, the discount factor (default=0.9).\n",
    "        categories      : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        labels          : List of the possible labels.\n",
    "        min_queries     : Int, the minimum number of queries the agent has to perform before being allowed to report a label.\n",
    "        d               : Int, the number of feature variables.\n",
    "        b               : Int, the number of class labels.\n",
    "        actions         : List of all actions.\n",
    "        actions_queries : List of query actions.\n",
    "        actions_report  : List of report actions.\n",
    "\n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.categories = categories\n",
    "        self.labels = labels\n",
    "        self.min_queries = min_queries\n",
    "        self.d = len(categories)\n",
    "        self.b = len(labels)\n",
    "        self.actions = range(self.d + len(labels))\n",
    "        self.actions_queries = range(self.d)\n",
    "        self.actions_report = [self.d + label for label in labels]\n",
    "        self.actor = Actor(input_size = np.sum(categories)+self.d, out = self.d+self.b)\n",
    "        self.critic = Critic(input_size = np.sum(categories)+self.d)\n",
    "        \n",
    "    def actions_probas(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Calculate the probabilities of the allowed actions at a state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state   : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        actions_probas  : torch.tensor of size #Allowed_actions, with the allowed actions probabilities.\n",
    "        actions_allowed : List of the allowed actions.\n",
    "        value           : 1D torch.tensor, the state value estimated by the value head of the Actor-Critic network.\n",
    "        \"\"\"\n",
    "        \n",
    "        input_tensor = torch.from_numpy(state.values_encoded)\n",
    "        value, actions_output = self.critic(input_tensor), self.actor(input_tensor)\n",
    "        if len(state.observed) < self.min_queries:\n",
    "            actions_allowed = list(state.unobserved)\n",
    "            \n",
    "        else:\n",
    "            actions_allowed = list(state.unobserved) + [self.d+i for i in range(self.b)]\n",
    "            \n",
    "        actions_probas = F.softmax(actions_output[0, actions_allowed], dim=0)\n",
    "        return actions_probas, actions_allowed, value\n",
    "        \n",
    "    def action(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose an action at a state by sampling from the current stochastic policy.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state  : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        action           : Int in {0, ..., d-1, d, d+1}, action sampled from the stochastic policy of the Actor.\n",
    "        action_log_proba : Float, the log probability correspoding to the performed action.\n",
    "        actions_probas   : List of length #Allowed_actions with the estimated probabilities of the allowed actions.\n",
    "        value            : 1D torch.tensor, the state value estimated by the value head of the Actor-Critic network.\n",
    "        \"\"\"\n",
    "        \n",
    "        actions_probas, actions_allowed, value = self.actions_probas(state)\n",
    "        m = Categorical(actions_probas)\n",
    "        index = m.sample()\n",
    "        action, action_log_prob = actions_allowed[index], m.log_prob(index)\n",
    "        return action, action_log_prob, actions_probas, value\n",
    "    \n",
    "    def action_greedy(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose the action maximizing the stochastic policy at the state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state  : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions_probas, actions_allowed, _ = self.actions_probas(state)\n",
    "            return actions_allowed[torch.argmax(actions_probas).item()]\n",
    "        \n",
    "        \n",
    "    def train(self, env, n_train=1000, lr_critic=3e-4, coef=0.1, max_j=10, delta=1e-2, tol=1e-3, log_dir='runs_trpo/', n_save=1000, path_save='trpo_weights/',\n",
    "              max_step=8, lambd=1e-3, clip_grad_critic=.1, clip_grad_actor=.1):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Explore the environment and train the agent.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env       : Object of class Environment.\n",
    "        n_train   : Int, number of training episodes.\n",
    "        lr        : Float, the learning rate.\n",
    "        log_dir   : String, path of the folder where tensorboard events are stored.\n",
    "        n_save    : Int, the number of episodes between two consecutive saved models.\n",
    "        path_save : String, path of the directory where the models weights will be stored.\n",
    "        max_step  : Int, the number of steps in an episode between two consecutive parameters update.\n",
    "        lambd     : Float, the entropy loss parameter.\n",
    "        clip_grad : Float in [0, 1] clipping the norm of the gradient to avoid over-optimistic updates.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "        if not os.path.isdir(path_save):\n",
    "            os.mkdir(path_save)\n",
    "        \n",
    "        optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        optimizer_actor = NaturalGradient(self.actor.parameters(), input_size=self.actor.input_size, out=self.actor.out, coef=coef, \n",
    "                                          delta=delta, tol=tol, min_queries=self.min_queries, d=self.d, b=self.b, max_j=max_j)\n",
    "        it = 0\n",
    "        for episode in range(n_train):\n",
    "            env.reset()\n",
    "            state = deepcopy(env.state)\n",
    "            episode_len = 0\n",
    "            episode_rewards = 0\n",
    "            while not env.done:\n",
    "                rewards, values, log_probs, probs, states = [], [], [], [], []\n",
    "                step = 0\n",
    "                entropy = 0\n",
    "                while (not env.done) and (step <= max_step):\n",
    "                    action, action_log_prob, actions_probas, value = self.action(state)\n",
    "                    reward, next_state, _ = env.step(action)\n",
    "                    episode_rewards += reward\n",
    "                    states.append(state)\n",
    "                    rewards.append(reward)\n",
    "                    values.append(value)\n",
    "                    probs.append(actions_probas)\n",
    "                    log_probs.append(action_log_prob.reshape(1, 1))\n",
    "                    state = deepcopy(next_state)\n",
    "                    entropy += -(actions_probas*torch.log(actions_probas)).sum()\n",
    "                    step += 1\n",
    "                    it += 1\n",
    "                    episode_len += 1\n",
    "\n",
    "                R = torch.tensor(0, dtype=torch.float32) if env.done else self.actor_critic(torch.from_numpy(state.values_encoded))[0].detach()\n",
    "                size = len(values)\n",
    "                values_target = [0 for i in range(size)]\n",
    "                for t in range(size-1, -1, -1):\n",
    "                    R = torch.tensor(rewards[t], dtype=torch.float32) + self.gamma*R\n",
    "                    values_target[t] = R.reshape(1, 1)\n",
    "\n",
    "                values_target = torch.cat(values_target)\n",
    "                values_current = torch.cat(values)\n",
    "                log_probs = torch.cat(log_probs)\n",
    "                probs_chosen = torch.exp(log_probs)\n",
    "\n",
    "                advantage = values_target - values_current\n",
    "                critic_loss = 0.5*(advantage**2).mean()\n",
    "                entropy_loss = -entropy/size\n",
    "#                 actor_loss = -(log_probs*advantage.detach()).mean() + lambd*entropy_loss\n",
    "                actor_loss = -((probs_chosen/probs_chosen.detach())*advantage.detach()).mean() + lambd*entropy_loss\n",
    "                # Optimize the Critic\n",
    "                optimizer_critic.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.critic.parameters(), clip_grad_critic)\n",
    "                optimizer_critic.step()\n",
    "                # Optimize the Actor\n",
    "                optimizer_actor.zero_grad()\n",
    "                actor_loss.backward(create_graph=True)\n",
    "                nn.utils.clip_grad_norm_(self.actor.parameters(), clip_grad_actor)\n",
    "                optimizer_actor.compute_grads(probs)\n",
    "                _, kl = optimizer_actor.step(probs, states)\n",
    "\n",
    "                writer.add_scalar('Losses/Actor', actor_loss.item(), it)\n",
    "                writer.add_scalar('Losses/Critic', critic_loss.item(), it)\n",
    "                writer.add_scalar('Losses/Entropy', entropy_loss, it)\n",
    "                writer.add_scalar('KL', kl, it)\n",
    "                \n",
    "            writer.add_scalar('Episode/Return', episode_rewards, episode)\n",
    "            writer.add_scalar('Episode/Length', episode_len, episode)\n",
    "            if episode%n_save == 0:\n",
    "                print('Episode : %d' %(episode))\n",
    "                self.save_weights(path_save + 'trpo_weights_' + str(episode) + '.pth')\n",
    "                        \n",
    "        writer.close()\n",
    "        \n",
    "    def predict(self, env, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Predict the label of a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        env.reset(data_point)\n",
    "        state = env.state\n",
    "        while not env.done:\n",
    "            action = self.action_greedy(state)\n",
    "            env.step(action)\n",
    "            state = env.state\n",
    "        \n",
    "        return action%self.d\n",
    "        \n",
    "    def test(self, env, n_test=1000):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Test the agent on n_test data points generated by env.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env      : Object of class Environment.\n",
    "        n_test   : Int, number of data points to test the agent on.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        accuracy : FLoat in [0, 1], the accuracy of the agent on this test.\n",
    "        \"\"\"\n",
    "        \n",
    "        valids = 0\n",
    "        for i in range(n_test):\n",
    "            data_point = env.generate()\n",
    "            env.reset(data_point)\n",
    "            label_pred, label_true = self.predict(env, data_point), env.label\n",
    "            valids += (label_pred==label_true)\n",
    "            \n",
    "        return valids/n_test\n",
    "    \n",
    "    def save_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Save the agents q-network weights.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        torch.save(self.actor.state_dict(), path)\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Load the weights of a q-network.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.actor.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def children(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Give the possible outcomes of taking the greedy policy at the considered state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        children : Set of objects of class State.\n",
    "        action   : Int, action taken at state with the agent policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        children = []\n",
    "        action = self.action_greedy(state)\n",
    "        if action >= self.d: return children, action\n",
    "        for category in range(self.categories[action]):\n",
    "            values = state.values.copy()\n",
    "            values[action] = category\n",
    "            children.append(StateDQN(values, state.encoder, self.categories))\n",
    "\n",
    "        return children, action\n",
    "    \n",
    "    def build_string_state(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision (with parentheses) starting from state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        l, action = self.children(state)\n",
    "        if action >= self.d: return str(self.action_greedy(state)%self.d) + ''\n",
    "        string = ''\n",
    "        for child in l:\n",
    "            string += '(X_' + str(action) + '=' + str(child.values[action]) + ' ' + self.build_string_state(child) + ') '\n",
    "\n",
    "        return string\n",
    "    \n",
    "    def build_string(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        return '( ' + self.build_string_state(StateDQN([np.NaN for i in range(self.d)], encoder, self.categories)) + ')'\n",
    "    \n",
    "    def plot_tree(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Plot the agent's decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        nltk tree object, helpful to visualize the agent's decision tree policy.\n",
    "        \"\"\"\n",
    "\n",
    "        return Tree.fromstring(self.build_string(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [3, 5, 3, 3, 3, 5, 3, 5, 3, 3, 4, 3, 3, 3, 5]\n",
    "# categories=[4, 3, 3, 3, 2, 4]\n",
    "labels = [0, 1]\n",
    "d, b = len(categories), len(labels)\n",
    "actions = range(d + len(labels))\n",
    "\n",
    "# Feature variables are independent and uniform\n",
    "generator = dict([(i, np.full(categories[i], 1/categories[i])) for i in range(len(categories))])\n",
    "\n",
    "# Each query action costs -1\n",
    "rewards_queries = dict([(i, -.5) for i in range(len(categories))])\n",
    "\n",
    "# Define ethe encoder\n",
    "encoder = Encoder(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvironmentDQN(generator, rewards_queries, encoder, r_plus=5, r_minus=-5, split=4)\n",
    "agent = AgentTRPO(categories=categories, min_queries=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = []\n",
    "# for i in range(10000):\n",
    "#     data_point = env.generate()\n",
    "#     l.append(env.concept(data_point))\n",
    "\n",
    "# print('Fraction of 1 labeled points : %.5f' %(np.sum(l)/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0\n",
      "Episode : 200\n",
      "Episode : 400\n",
      "Episode : 600\n",
      "Episode : 800\n",
      "Episode : 1000\n",
      "Episode : 1200\n",
      "Episode : 1400\n",
      "Episode : 1600\n",
      "Episode : 1800\n",
      "Episode : 2000\n",
      "Episode : 2200\n",
      "Episode : 2400\n",
      "Episode : 2600\n",
      "Episode : 2800\n",
      "Episode : 3000\n",
      "Episode : 3200\n",
      "Episode : 3400\n",
      "Episode : 3600\n",
      "Episode : 3800\n",
      "Episode : 4000\n",
      "Episode : 4200\n",
      "Episode : 4400\n",
      "Episode : 4600\n",
      "Episode : 4800\n",
      "Episode : 5000\n",
      "Episode : 5200\n",
      "Episode : 5400\n",
      "Episode : 5600\n",
      "Episode : 5800\n",
      "Episode : 6000\n",
      "Episode : 6200\n",
      "Episode : 6400\n",
      "Episode : 6600\n",
      "Episode : 6800\n",
      "Episode : 7000\n",
      "Episode : 7200\n",
      "Episode : 7400\n",
      "Episode : 7600\n",
      "Episode : 7800\n",
      "Episode : 8000\n",
      "Episode : 8200\n",
      "Episode : 8400\n",
      "Episode : 8600\n",
      "Episode : 8800\n",
      "Episode : 9000\n",
      "Episode : 9200\n",
      "Episode : 9400\n",
      "Episode : 9600\n",
      "Episode : 9800\n",
      "Episode : 10000\n",
      "Episode : 10200\n",
      "Episode : 10400\n",
      "Episode : 10600\n",
      "Episode : 10800\n",
      "Episode : 11000\n",
      "Episode : 11200\n",
      "Episode : 11400\n",
      "Episode : 11600\n",
      "Episode : 11800\n",
      "Episode : 12000\n",
      "Episode : 12200\n",
      "Episode : 12400\n",
      "Episode : 12600\n",
      "Episode : 12800\n",
      "Episode : 13000\n",
      "Episode : 13200\n",
      "Episode : 13400\n",
      "Episode : 13600\n",
      "Episode : 13800\n",
      "Episode : 14000\n",
      "Episode : 14200\n",
      "Episode : 14400\n",
      "Episode : 14600\n",
      "Episode : 14800\n",
      "Episode : 15000\n",
      "Episode : 15200\n",
      "Episode : 15400\n",
      "Episode : 15600\n",
      "Episode : 15800\n",
      "Episode : 16000\n",
      "Episode : 16200\n",
      "Episode : 16400\n",
      "Episode : 16600\n",
      "Episode : 16800\n",
      "Episode : 17000\n",
      "Episode : 17200\n",
      "Episode : 17400\n",
      "Episode : 17600\n",
      "Episode : 17800\n",
      "Episode : 18000\n",
      "Episode : 18200\n",
      "Episode : 18400\n",
      "Episode : 18600\n",
      "Episode : 18800\n",
      "Episode : 19000\n",
      "Episode : 19200\n",
      "Episode : 19400\n",
      "Episode : 19600\n",
      "Episode : 19800\n",
      "Episode : 20000\n",
      "Episode : 20200\n",
      "Episode : 20400\n",
      "Episode : 20600\n",
      "Episode : 20800\n",
      "Episode : 21000\n",
      "Episode : 21200\n",
      "Episode : 21400\n",
      "Episode : 21600\n",
      "Episode : 21800\n",
      "Episode : 22000\n",
      "Episode : 22200\n",
      "Episode : 22400\n",
      "Episode : 22600\n",
      "Episode : 22800\n",
      "Episode : 23000\n",
      "Episode : 23200\n",
      "Episode : 23400\n",
      "Episode : 23600\n",
      "Episode : 23800\n",
      "Episode : 24000\n",
      "Episode : 24200\n",
      "Episode : 24400\n",
      "Episode : 24600\n",
      "Episode : 24800\n",
      "Episode : 25000\n",
      "Episode : 25200\n",
      "Episode : 25400\n",
      "Episode : 25600\n",
      "Episode : 25800\n",
      "Episode : 26000\n",
      "Episode : 26200\n",
      "Episode : 26400\n",
      "Episode : 26600\n",
      "Episode : 26800\n",
      "Episode : 27000\n",
      "Episode : 27200\n",
      "Episode : 27400\n",
      "Episode : 27600\n",
      "Episode : 27800\n",
      "Episode : 28000\n",
      "Episode : 28200\n",
      "Episode : 28400\n",
      "Episode : 28600\n",
      "Episode : 28800\n",
      "Episode : 29000\n",
      "Episode : 29200\n",
      "Episode : 29400\n",
      "Episode : 29600\n",
      "Episode : 29800\n",
      "Episode : 30000\n",
      "Episode : 30200\n",
      "Episode : 30400\n",
      "Episode : 30600\n",
      "Episode : 30800\n",
      "Episode : 31000\n",
      "Episode : 31200\n",
      "Episode : 31400\n",
      "Episode : 31600\n",
      "Episode : 31800\n",
      "Episode : 32000\n",
      "Episode : 32200\n",
      "Episode : 32400\n",
      "Episode : 32600\n",
      "Episode : 32800\n",
      "Episode : 33000\n",
      "Episode : 33200\n",
      "Episode : 33400\n",
      "Episode : 33600\n",
      "Episode : 33800\n",
      "Episode : 34000\n",
      "Episode : 34200\n",
      "Episode : 34400\n",
      "Episode : 34600\n",
      "Episode : 34800\n",
      "Episode : 35000\n",
      "Episode : 35200\n",
      "Episode : 35400\n",
      "Episode : 35600\n",
      "Episode : 35800\n",
      "Episode : 36000\n",
      "Episode : 36200\n",
      "Episode : 36400\n",
      "Episode : 36600\n",
      "Episode : 36800\n",
      "Episode : 37000\n",
      "Episode : 37200\n",
      "Episode : 37400\n",
      "Episode : 37600\n",
      "Episode : 37800\n",
      "Episode : 38000\n",
      "Episode : 38200\n",
      "Episode : 38400\n",
      "Episode : 38600\n",
      "Episode : 38800\n",
      "Episode : 39000\n",
      "Episode : 39200\n",
      "Episode : 39400\n",
      "Episode : 39600\n",
      "Episode : 39800\n",
      "Episode : 40000\n",
      "Episode : 40200\n",
      "Episode : 40400\n",
      "Episode : 40600\n",
      "Episode : 40800\n",
      "Episode : 41000\n",
      "Episode : 41200\n",
      "Episode : 41400\n",
      "Episode : 41600\n",
      "Episode : 41800\n",
      "Episode : 42000\n",
      "Episode : 42200\n",
      "Episode : 42400\n",
      "Episode : 42600\n",
      "Episode : 42800\n"
     ]
    }
   ],
   "source": [
    "# agent.train(env, n_train=100000, delta=1e-2, tol=1e-3, lr_critic=3e-4, coef=0.5, lambd=1, max_step=30,\n",
    "#             log_dir='runs_trpo_min_queries_4/', path_save='trpo_weights_min_queries_4/', max_j=10)\n",
    "agent.train(env, n_train=100000, delta=1e-3, tol=1e-3, lr_critic=1e-4, coef=0.5, lambd=1, max_step=30,\n",
    "            log_dir='runs_trpo_min_queries_4/', path_save='trpo_weights_min_queries_4/', max_j=10, n_save=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_weights('trpo_weights_min_queries_4/trpo_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.test(env, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = StateDQN([np.NaN for i in range(15)], encoder, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actions_probas(state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred, labels_true = [], []\n",
    "for i in range(10000):\n",
    "    data_point = env.generate()\n",
    "    labels_pred.append(agent.predict(env, data_point))\n",
    "    labels_true.append(env.concept(data_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of true ones      : %d' %np.sum(labels_true))\n",
    "print('Number of predicted ones : %d' %np.sum(labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for i in range(1000):\n",
    "    data_point = env.generate()\n",
    "    env.reset(data_point)\n",
    "    state = env.state\n",
    "    depth = 0\n",
    "    while not env.done:\n",
    "        action = agent.action_greedy(state)\n",
    "        _, state, _ = env.step(action)\n",
    "        depth += 1\n",
    "        \n",
    "    depths.append(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(depths, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_tree(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(10, 10)\n",
    "b = torch.zeros(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.new_tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
