{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from queue import Queue\n",
    "from copy import deepcopy\n",
    "from nltk import Tree\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'next_state_unallowed'))\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    \n",
    "    \"\"\"\n",
    "    Class for the uniform experience replay memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size, b=3):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -------------\n",
    "        Constructor of class Memory.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        -------------\n",
    "        max_size   : Int, the maximum size of the replay memory.\n",
    "        buffer     : collections.deque object of maximum length max_size, the container representing the replay memory.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b = b\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -------------\n",
    "        Add experience to the replay buffer.\n",
    "        \n",
    "        Parameters\n",
    "        -------------\n",
    "        experience : 4-tuple representing a transition (state, action, reward, next_state).\n",
    "                     - state      : Object of class State representing the state.\n",
    "                     - action     : Int in {0, ..., d-1, d, d+1}, the action.\n",
    "                     - reward     : Float, the reward.\n",
    "                     - next_state : Object of class State representing the next state.\n",
    "                     \n",
    "        Returns\n",
    "        -------------\n",
    "        \"\"\"\n",
    "        \n",
    "        next_state_unallowed = np.zeros(len(next_state.categories) + self.b, dtype=np.float32)\n",
    "        next_state_unallowed[next_state.observed] = 1\n",
    "        self.buffer.append((torch.from_numpy(state.values_encoded), [action], torch.tensor([reward], dtype=torch.float32), \n",
    "                            torch.from_numpy(next_state.values_encoded), torch.from_numpy(next_state_unallowed).view(1, -1)))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -------------\n",
    "        Randomly sample \"batch_size\" experiences from the replay buffer.\n",
    "        \n",
    "        Parameters\n",
    "        -------------\n",
    "        batch_size : Int, the number of experiences to sample.\n",
    "        \n",
    "        Returns\n",
    "        -------------\n",
    "        Named tuple representing the sampled batch.\n",
    "        \"\"\"\n",
    "        \n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class represeting the one-hot encoder of the states.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Encoder.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        d          : Int, the input dimension.\n",
    "        dim        : Int, the one-hot encoded representation dimension\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.categories = categories\n",
    "        self.d = len(categories)\n",
    "        self.dim = np.sum(categories) + self.d\n",
    "        \n",
    "    def transform(self, state_values):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the vector state representation with dummies.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        state_values : List of length d where the ith entry is either NaN or the the feature value.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        state_one_hot : 2D np.array representing the one-hot encoded state.\n",
    "        \"\"\"\n",
    "    \n",
    "        s = 0\n",
    "        state_one_hot = np.zeros((1, self.dim), dtype = np.float32)\n",
    "        for i, value in enumerate(state_values):\n",
    "            if np.isnan(value):\n",
    "                state_one_hot[0, self.categories[i] + s] = 1\n",
    "\n",
    "            else:\n",
    "                state_one_hot[0, value + s] = 1\n",
    "\n",
    "            s += self.categories[i]+1\n",
    "\n",
    "        return state_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateDQN:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing a state, it also serves as Node representation\n",
    "    for our Breadth First Search\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, values, encoder, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class State.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        values     : List of length d (Input dimension):\n",
    "                         - values[i] = NaN if i is an unobsorved feature.\n",
    "                         - values[i] = value of feature i if it is observed.\n",
    "        encocer    : sklearn.preprocessing._encoders.OneHotEncoder object.\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        observed   : List containing the observed features at this state.\n",
    "        unobserved : List containing the unobserved features at this state.\n",
    "        empty      : Boolean, whether it is the empty state or not.\n",
    "        complete   : Boolean, whether all features are observed or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        d = len(values)\n",
    "        values_nans = np.isnan(values)\n",
    "        self.encoder = encoder  # One-hot encoder, used in the approximate RL framework for state representation.\n",
    "        self.values = values\n",
    "        self.values_encoded = self.encode()\n",
    "        self.categories = categories\n",
    "        self.observed = np.arange(d)[np.invert(values_nans)]\n",
    "        self.unobserved = np.arange(d)[values_nans]  # These are also the allowed query actions at this state\n",
    "        self.empty = (len(self.observed) == 0)\n",
    "        self.complete = (len(self.unobserved) == 0)\n",
    "        \n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the state with dummy variables. To be used when a one-hot encoder is defined.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        np.array of shape (1, #one_hot_representation_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.encoder.transform(self.values)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=25, out=8):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        Constructor of DQNetwork class.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        input_size : Int, the one-hot encoding representation dimension.\n",
    "        out        : Int, output dimension, equal to the number of possible actions.\n",
    "        fc_1       : nn.Linear, first fully connected layer.\n",
    "        fc_2       : nn.Linear, second fully connected layer.\n",
    "        output     : nn.Linear, output fully connected layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_size, 64)\n",
    "        self.fc_2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        The forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        x : torch.tensor of dimension (batch_size, input_size)\n",
    "        \n",
    "        Returns\n",
    "        ---------------\n",
    "        torch.tensor of dimension (batch_size, out)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentDQN:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing the environment, it can generate data points that start each episode,\n",
    "    keep track of the current state, return the reward of an action taken at the current state,\n",
    "    and transition to the next corresponding state.\n",
    "    \"\"\"\n",
    "    \n",
    "    episode = 0\n",
    "    \n",
    "    def __init__(self, generator, rewards_queries, encoder, r_plus=5, r_minus=-5, split_1=4, split_2=3):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Environment.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        generator       : Dict, - keys   : Feature variables.\n",
    "                                - values : List of probability masses of each category of the corresponding feature.\n",
    "        rewards_queries : Dict, - keys   : Feature variables.\n",
    "                                - values : Reward of querying the value of the corresponding feature.\n",
    "        encoder         : Object of class Encoder, the encoder mapping states to their one-hot representation.\n",
    "        r_plus          : Int, reward of a correct report (default=5).\n",
    "        r_minus         : Int, reward of an incorrect report (default=-5).\n",
    "        split           : Int, the split point we use to define our concept.\n",
    "        d               : Int, the number of feature variables.\n",
    "        data_point      : List of length d, the data point starting the episode.\n",
    "        label           : Boolean, the true label of data_point.\n",
    "        state           : Object of class State, the current state.\n",
    "        done            : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.generator = generator\n",
    "        self.categories = [len(v) for v in self.generator.values()]\n",
    "        self.d = len(self.categories)\n",
    "        self.rewards_queries = rewards_queries\n",
    "        self.encoder = encoder\n",
    "        self.r_plus = r_plus\n",
    "        self.r_minus = r_minus\n",
    "        self.split_1 = split_1\n",
    "        self.split_2 = split_2\n",
    "        \n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Generate a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        List with the values of each feature, it represents the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        return [np.random.choice(self.categories[i], p=self.generator[i]) for i in range(self.d)]\n",
    "    \n",
    "    def concept_1(self, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Define the concept labeling the data points. we can define it as a decision tree for example.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        Int in {0, 1}, the label of the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        label = True\n",
    "        i = 0\n",
    "        while label and i <= self.d-1:\n",
    "            if data_point[i] >= self.split_1:\n",
    "                label = False\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def concept_2(self, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Define the concept labeling the data points. we can define it as a decision tree for example.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        Int in {0, 1}, the label of the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        label = True\n",
    "        i = 0\n",
    "        while label and i <= self.d-1:\n",
    "            if data_point[i] == self.split_2:\n",
    "                label = False\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def reset(self, data_point=None):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Reset the environment to start a new episode. If data_point is specified, start the episode from it,\n",
    "        otherwise generate it.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label (default=None).\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_point = self.generate() if data_point is None else data_point\n",
    "        b = np.random.binomial(1, 1/(1 + np.exp(-(self.episode - 5e4)/5e3)))\n",
    "        if b == 0:\n",
    "            self.label = self.concept_1(self.data_point)\n",
    "            \n",
    "        else:\n",
    "            self.label = self.concept_2(self.data_point)\n",
    "            \n",
    "        self.state = StateDQN([np.NaN for i in range(self.d)], self.encoder, categories=self.categories)\n",
    "        self.done = False\n",
    "        env.episode += 1\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Interract with the environment through an action taken at the current state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}, \n",
    "                 - 0, ..., d-1 represent query actions.\n",
    "                 - d, d+1 represent report actions.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        reward     : Int, the reward of taking this action at the current state.\n",
    "        next_state : Object of class State, the next state.\n",
    "        done       : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Treating query actions.\n",
    "        if action <= self.d-1:\n",
    "            # If it is an allowed query action.\n",
    "            if np.isnan(self.state.values[action]):\n",
    "                reward = self.rewards_queries[action]\n",
    "                values = self.state.values\n",
    "                values[action] = self.data_point[action] # Reveal the value of the queried feature in the data point.\n",
    "                self.state = StateDQN(values, self.encoder, self.categories)\n",
    "                \n",
    "            # If this query action is not allowed.\n",
    "            else:\n",
    "                print('unallowed')\n",
    "            \n",
    "        # Treating report actions.\n",
    "        else:\n",
    "            reward = self.r_plus if (action%self.d) == self.label else self.r_minus\n",
    "            self.done = True\n",
    "            \n",
    "        return reward, self.state, self.done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDQN:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class describing a DQN agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=.9, categories=[4, 3, 3, 3, 2, 4], labels=[0, 1], max_size_queries=int(1e4), max_size_reports=int(1e3)):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class AgentDQN.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        gamma            : Float in ]0, 1[, the discount factor (default=0.9).\n",
    "        categories       : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        labels           : List of the possible labels.\n",
    "        max_size         : Int, the maximum size of the experience replay memory.\n",
    "        d                : Int, the number of feature variables.\n",
    "        b                : Int, the number of class labels.\n",
    "        actions          : List of all actions.\n",
    "        actions_queries  : List of query actions.\n",
    "        actions_report   : List of report actions.\n",
    "        memory_queries   : Object of class Memory, replay buffer containing experiences with query actions only.\n",
    "        memory_reports   : Object of class Memory, replay buffer containing experiences with report actions only.\n",
    "        q_network        : Object of class DQNetwork, the current q-network estimating the Q-function.\n",
    "        q_network_target : Object of class DQNetwork, the target q-network, it should have the same architecture as q_network.\n",
    "\n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.categories = categories\n",
    "        self.labels = labels\n",
    "        self.max_size_queries = max_size_queries\n",
    "        self.max_size_reports = max_size_reports\n",
    "        self.d = len(categories)\n",
    "        self.b = len(labels)\n",
    "        self.actions = range(self.d + len(labels))\n",
    "        self.actions_queries = range(self.d)\n",
    "        self.actions_report = [self.d + label for label in labels]\n",
    "        self.memory_queries = Memory(max_size_queries, self.b)\n",
    "        self.memory_reports = Memory(max_size_reports, self.b)\n",
    "        self.q_network = DQNetwork(input_size=np.sum(categories)+self.d, out=self.d+self.b)\n",
    "        self.q_network_target = DQNetwork(input_size=np.sum(categories)+self.d, out=self.d+self.b)\n",
    "        self.accuracies = []\n",
    "        \n",
    "    def action_explore(self, state, epsilon=.1):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose a query action and a report action at a state according to the epsilon-greedy policy.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state   : Object of class State.\n",
    "        epsilon : Float in [0, 1], the probability of taking an action according to the greedy policy.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        query  : Int in {0, ..., d-1}\n",
    "        report : Int in {d, d+1}\n",
    "        \"\"\"\n",
    "        \n",
    "        bern = np.random.binomial(1, epsilon)\n",
    "        \n",
    "        # Use conditions to avoid unnecessary forward passes on the Q-network.\n",
    "        if bern == 1:\n",
    "            query = np.random.choice(state.unobserved)\n",
    "            report = np.random.randint(self.d, self.d + self.b)\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(torch.from_numpy(state.values_encoded))\n",
    "                # Consider only q-values of allowed queries (for queries)\n",
    "                q_queries_allowed = q_values[0, state.unobserved]\n",
    "                # map argmax to the corresponding index in the allowed queries.\n",
    "                query = state.unobserved[torch.argmax(q_queries_allowed).item()]\n",
    "                # Take the greedy report action.\n",
    "                report = torch.argmax(q_values[0, self.d:]).item()+self.d\n",
    "            \n",
    "        return query, report\n",
    "    \n",
    "    def action(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose an action at a state greedy policy.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state   : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}.\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(torch.from_numpy(state.values_encoded))\n",
    "            actions_allowed = list(state.unobserved) + [self.d+i for i in range(self.b)]\n",
    "            q_values_allowed = q_values[0, actions_allowed]\n",
    "            return actions_allowed[torch.argmax(q_values_allowed).item()]\n",
    "    \n",
    "    def update_target(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -------------\n",
    "        Update the parameters of target_model with those of current_model\n",
    "\n",
    "        Parameters\n",
    "        -------------\n",
    "        current_model, target_model : torch models\n",
    "        \"\"\"\n",
    "        \n",
    "        self.q_network_target.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def pretrain(self, env, n_episodes):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Fill the memory buffers with experiences in a pretraining phase.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env        : Object of class EnvironmentDQN.\n",
    "        n_episodes : Int, number of episodes.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            env.reset()\n",
    "            state = deepcopy(env.state)\n",
    "            while not state.complete:\n",
    "                query, report = np.random.choice(state.unobserved), np.random.randint(self.d, self.d + self.b)\n",
    "                reward, next_state, _ = env.step(report)\n",
    "                self.memory_reports.add(state, report, reward, next_state)\n",
    "                reward, next_state, _ = env.step(query)\n",
    "                self.memory_queries.add(state, query, reward, next_state)\n",
    "                state = deepcopy(next_state)\n",
    "                \n",
    "            report = np.random.randint(self.d, self.d + self.b)\n",
    "            reward, next_state, _ = env.step(report)\n",
    "            self.memory_reports.add(state, report, reward, next_state)   \n",
    "            \n",
    "    def train(self, env, n_train=10000, n_pretrain=100, n_pretrain_ac=5000, epsilon_start=1, epsilon_stop=1e-4, decay_rate=1e-3, n_learn=5, batch_size=32, \n",
    "              lr=1e-4, log_dir='runs_dqn/', n_write=10, max_tau=50, double=True, n_save=1000):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Explore the environment and train the agent.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env           : Object of class EnvironmentDQN.\n",
    "        n_train       : Int, number of training episodes.\n",
    "        n_pretrain    : Int, number of pretraining episode.\n",
    "        epsilon_start : Float in [0, 1], the starting epsilon parameter of the epsilon-greedy policy, it should be high.\n",
    "        epsilon_stop  : Float in [0, 1], the last epsilon parameter of the epsilon-greedy policy, it should be small.\n",
    "        decay_rate    : Float in [0, 1], the decay rate of epsilon.\n",
    "        n_learn       : Int, the number of iterations between two consecutive learning phases.\n",
    "        batch_size    : Int, the batch size for both replay buffers.\n",
    "        lr            : Float, the learning rate.\n",
    "        log_dir       : String, path of the folder where tensorboard events are stored.\n",
    "        n_write       : Int, the number of iterations between two consecutive events writings.\n",
    "        max_tau       : Int, the number of iterations between two consecutive target network updates.\n",
    "        double        : Boolean, whether to use Double DQN or just DQN.\n",
    "        n_save        : Int, the number of episodes between two consecutive saved models.\n",
    "        path_save     : String, the path of the folder where weights will be saved.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "#         accuracies_window = deque(maxlen=1000)\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "#         if not os.path.isdir(path_save):\n",
    "#             os.mkdir(path_save)\n",
    "        \n",
    "        # Pretrain the agent.\n",
    "        self.pretrain(env, n_pretrain)\n",
    "        epsilon = epsilon_start\n",
    "        it = 0\n",
    "        optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        variables = {'losses' : [], 'accuracies' : deque(maxlen=1000)}\n",
    "        tau = 0\n",
    "        for episode in range(n_train):\n",
    "            env.reset()\n",
    "            state = deepcopy(env.state)\n",
    "            if episode >= n_pretrain_ac:\n",
    "                label_pred = self.predict(env, env.data_point)\n",
    "                label = env.label\n",
    "                variables['accuracies'].append(label==label_pred)\n",
    "                if len(variables['accuracies']) == 1000:\n",
    "#                     self.accuracies.append(np.mean(accuracies_window))\n",
    "                    writer.add_scalar('accuracy', np.mean(variables['accuracies']), it)\n",
    "                \n",
    "            env.reset(env.data_point)\n",
    "            while not state.complete:\n",
    "                query, report = self.action_explore(state, epsilon)\n",
    "                it += 1\n",
    "                tau += 1\n",
    "                epsilon = epsilon_stop + (epsilon_start - epsilon_stop)*np.exp(-decay_rate*it)\n",
    "                reward, next_state, _ = env.step(report)\n",
    "                self.memory_reports.add(state, report, reward, next_state)\n",
    "                reward, next_state, _ = env.step(query)\n",
    "                self.memory_queries.add(state, query, reward, next_state)\n",
    "                state = deepcopy(next_state)\n",
    "                \n",
    "                # Learning phase\n",
    "                if it%n_learn == 0:\n",
    "                    batch_queries = self.memory_queries.sample(batch_size)\n",
    "                    batch_reports = self.memory_reports.sample(batch_size)\n",
    "                    \n",
    "                    states_batch_queries, states_batch_reports = torch.cat(batch_queries.state), torch.cat(batch_reports.state)\n",
    "                    actions_batch_queries, actions_batch_reports = np.concatenate(batch_queries.action), np.concatenate(batch_reports.action)\n",
    "                    rewards_batch_queries, rewards_batch_reports = torch.cat(batch_queries.reward), torch.cat(batch_reports.reward)\n",
    "                    next_states_batch_queries, next_states_batch_reports = torch.cat(batch_queries.next_state), torch.cat(batch_reports.next_state)\n",
    "                    next_states_batch_unallowed = torch.cat(batch_queries.next_state_unallowed)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if double:\n",
    "                            q_values = self.q_network(next_states_batch_queries)\n",
    "                            q_values = q_values - 100*next_states_batch_unallowed\n",
    "                            actions_values = torch.argmax(q_values, dim=1)\n",
    "                            q_targets_queries = rewards_batch_queries + self.gamma*self.q_network_target(next_states_batch_queries)[np.arange(batch_size), actions_values].detach()\n",
    "\n",
    "                        else:\n",
    "                            q_values = self.q_network_target(next_states_batch_queries)\n",
    "                            q_values = q_values - 100*next_states_batch_unallowed\n",
    "                            q_targets_queries = rewards_batch_queries + self.gamma*torch.max(q_values, dim=1).values\n",
    "\n",
    "                    q_values_queries = self.q_network(states_batch_queries)[np.arange(batch_size), actions_batch_queries]\n",
    "                    q_targets_reports = rewards_batch_reports\n",
    "                    q_values_reports = self.q_network(states_batch_reports)[np.arange(batch_size), actions_batch_reports]\n",
    "                    \n",
    "                    q_targets = torch.cat((q_targets_queries, q_targets_reports))\n",
    "                    q_values = torch.cat((q_values_queries, q_values_reports))\n",
    "                    \n",
    "                    loss = F.mse_loss(q_values, q_targets)\n",
    "                    variables['losses'].append(loss.item())\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    if it%n_write == 0:\n",
    "                        writer.add_scalar('Loss', np.mean(variables['losses']), it)\n",
    "                        \n",
    "                if tau == max_tau:\n",
    "                    self.update_target()\n",
    "                    tau = 0\n",
    "                    \n",
    "            # The last action of an episode should be a report, take it according to the epsilon-greedy policy.\n",
    "            bern = np.random.binomial(1, epsilon)\n",
    "            if bern == 1:\n",
    "                report = np.random.randint(self.d, self.d + self.b)\n",
    "\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.q_network(torch.from_numpy(state.values_encoded))\n",
    "                    report = torch.argmax(q_values[0, self.d:]).item()+self.d\n",
    "                        \n",
    "            epsilon = epsilon_stop + (epsilon_start - epsilon_stop)*np.exp(-decay_rate*it)\n",
    "            reward, next_state, _ = env.step(report)\n",
    "            self.memory_reports.add(state, report, reward, next_state)\n",
    "            \n",
    "            if episode%n_save == 0:\n",
    "                print('Episode : %d, epsilon : %.3f' %(episode, epsilon))\n",
    "#                 self.save_weights(path_save + 'dqn_weights_' + str(episode) + '.pth')\n",
    "                        \n",
    "        writer.close()\n",
    "        \n",
    "    def predict(self, env, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Predict the label of a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env        : Object of class EnvironmentDQN.\n",
    "        data_point : List of length d, the data point to label.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        Int in {0, 1}, the predicted label.\n",
    "        \"\"\"\n",
    "        \n",
    "        env.reset(data_point)\n",
    "        state = env.state\n",
    "        while not env.done:\n",
    "            action = self.action(state)\n",
    "            env.step(action)\n",
    "            state = env.state\n",
    "        \n",
    "        return action%self.d\n",
    "        \n",
    "    def test(self, env, n_test=1000):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Test the agent on n_test data points generated by env.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env      : Object of class EnvironmentDQN.\n",
    "        n_test   : Int, number of data points to test the agent on.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        accuracy : FLoat in [0, 1], the accuracy of the agent on this test.\n",
    "        \"\"\"\n",
    "        \n",
    "        valids = 0\n",
    "        for i in range(n_test):\n",
    "            data_point, label = env.generate()\n",
    "            env.reset(data_point, label)\n",
    "            label_pred = self.predict(env, data_point)\n",
    "            valids += (label_pred==label)\n",
    "            \n",
    "        return valids/n_test\n",
    "    \n",
    "    def save_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Save the agents q-network weights.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        torch.save(self.q_network.state_dict(), path)\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Load the weights of a q-network.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.q_network.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def children(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Give the possible outcomes of taking the greedy policy at the considered state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class StateDQN.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        children : Set of objects of class State.\n",
    "        action   : Int, action taken at state with the agent policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        children = []\n",
    "        action = self.action(state)\n",
    "        if action >= self.d: return children, action\n",
    "        for category in range(self.categories[action]):\n",
    "            values = state.values.copy()\n",
    "            values[action] = category\n",
    "            children.append(StateDQN(values, state.encoder, self.categories))\n",
    "\n",
    "        return children, action\n",
    "    \n",
    "    def build_string_state(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision (with parentheses) starting from state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class StateDQN.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        l, action = self.children(state)\n",
    "        if action >= self.d: return str(self.action(state)%self.d) + ''\n",
    "        string = ''\n",
    "        for child in l:\n",
    "            string += '(X_' + str(action) + '=' + str(child.values[action]) + ' ' + self.build_string_state(child) + ') '\n",
    "\n",
    "        return string\n",
    "    \n",
    "    def build_string(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        encoder : Object of class Encoder, the encoder mapping states to their one-hot representation.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        return '( ' + self.build_string_state(StateDQN([np.NaN for i in range(self.d)], encoder, self.categories)) + ')'\n",
    "    \n",
    "    def plot_tree(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Plot the agent's decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        encoder : Object of class Encoder, the encoder mapping states to their one-hot representation.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        nltk tree object, helpful to visualize the agent's decision tree policy.\n",
    "        \"\"\"\n",
    "\n",
    "        return Tree.fromstring(self.build_string(encoder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [3, 5, 3, 3, 3, 5, 3, 5, 3, 3, 4, 3, 3, 3, 5]\n",
    "d = len(categories)\n",
    "labels = [0, 1]\n",
    "actions = range(d + len(labels))\n",
    "\n",
    "# Feature variables are independent and uniform\n",
    "generator = dict([(i, np.full(categories[i], 1/categories[i])) for i in range(len(categories))])\n",
    "\n",
    "# Each query action costs -1\n",
    "rewards_queries = dict([(i, -.5) for i in range(len(categories))])\n",
    "\n",
    "# Define the one-hot encoder.\n",
    "encoder = Encoder(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvironmentDQN(generator, rewards_queries, encoder, r_plus=5, r_minus=-5, split_1=4, split_2=3)\n",
    "agent = AgentDQN(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of 1 labeled points : 0.40150\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for i in range(10000):\n",
    "    data_point = env.generate()\n",
    "    l.append(env.concept_1(data_point))\n",
    "\n",
    "print('Fraction of 1 labeled points : %.5f' %(np.sum(l)/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of 1 labeled points : 0.31080\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for i in range(10000):\n",
    "    data_point = env.generate()\n",
    "    l.append(env.concept_2(data_point))\n",
    "\n",
    "print('Fraction of 1 labeled points : %.5f' %(np.sum(l)/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0, epsilon : 1.000\n",
      "Episode : 1000, epsilon : 0.970\n",
      "Episode : 2000, epsilon : 0.942\n",
      "Episode : 3000, epsilon : 0.914\n",
      "Episode : 4000, epsilon : 0.887\n",
      "Episode : 5000, epsilon : 0.861\n",
      "Episode : 6000, epsilon : 0.835\n",
      "Episode : 7000, epsilon : 0.811\n",
      "Episode : 8000, epsilon : 0.787\n",
      "Episode : 9000, epsilon : 0.763\n",
      "Episode : 10000, epsilon : 0.741\n",
      "Episode : 11000, epsilon : 0.719\n",
      "Episode : 12000, epsilon : 0.698\n",
      "Episode : 13000, epsilon : 0.677\n",
      "Episode : 14000, epsilon : 0.657\n",
      "Episode : 15000, epsilon : 0.638\n",
      "Episode : 16000, epsilon : 0.619\n",
      "Episode : 17000, epsilon : 0.601\n",
      "Episode : 18000, epsilon : 0.583\n",
      "Episode : 19000, epsilon : 0.566\n",
      "Episode : 20000, epsilon : 0.549\n",
      "Episode : 21000, epsilon : 0.533\n",
      "Episode : 22000, epsilon : 0.517\n",
      "Episode : 23000, epsilon : 0.502\n",
      "Episode : 24000, epsilon : 0.487\n",
      "Episode : 25000, epsilon : 0.472\n",
      "Episode : 26000, epsilon : 0.458\n",
      "Episode : 27000, epsilon : 0.445\n",
      "Episode : 28000, epsilon : 0.432\n",
      "Episode : 29000, epsilon : 0.419\n",
      "Episode : 30000, epsilon : 0.407\n",
      "Episode : 31000, epsilon : 0.395\n",
      "Episode : 32000, epsilon : 0.383\n",
      "Episode : 33000, epsilon : 0.372\n",
      "Episode : 34000, epsilon : 0.361\n",
      "Episode : 35000, epsilon : 0.350\n",
      "Episode : 36000, epsilon : 0.340\n",
      "Episode : 37000, epsilon : 0.330\n",
      "Episode : 38000, epsilon : 0.320\n",
      "Episode : 39000, epsilon : 0.310\n",
      "Episode : 40000, epsilon : 0.301\n",
      "Episode : 41000, epsilon : 0.292\n",
      "Episode : 42000, epsilon : 0.284\n",
      "Episode : 43000, epsilon : 0.275\n",
      "Episode : 44000, epsilon : 0.267\n",
      "Episode : 45000, epsilon : 0.259\n",
      "Episode : 46000, epsilon : 0.252\n",
      "Episode : 47000, epsilon : 0.244\n",
      "Episode : 48000, epsilon : 0.237\n",
      "Episode : 49000, epsilon : 0.230\n",
      "Episode : 50000, epsilon : 0.223\n",
      "Episode : 51000, epsilon : 0.217\n",
      "Episode : 52000, epsilon : 0.210\n",
      "Episode : 53000, epsilon : 0.204\n",
      "Episode : 54000, epsilon : 0.198\n",
      "Episode : 55000, epsilon : 0.192\n",
      "Episode : 56000, epsilon : 0.186\n",
      "Episode : 57000, epsilon : 0.181\n",
      "Episode : 58000, epsilon : 0.176\n",
      "Episode : 59000, epsilon : 0.170\n",
      "Episode : 60000, epsilon : 0.165\n",
      "Episode : 61000, epsilon : 0.160\n",
      "Episode : 62000, epsilon : 0.156\n",
      "Episode : 63000, epsilon : 0.151\n",
      "Episode : 64000, epsilon : 0.147\n",
      "Episode : 65000, epsilon : 0.142\n",
      "Episode : 66000, epsilon : 0.138\n",
      "Episode : 67000, epsilon : 0.134\n",
      "Episode : 68000, epsilon : 0.130\n",
      "Episode : 69000, epsilon : 0.126\n",
      "Episode : 70000, epsilon : 0.123\n",
      "Episode : 71000, epsilon : 0.119\n",
      "Episode : 72000, epsilon : 0.115\n",
      "Episode : 73000, epsilon : 0.112\n",
      "Episode : 74000, epsilon : 0.109\n",
      "Episode : 75000, epsilon : 0.105\n",
      "Episode : 76000, epsilon : 0.102\n",
      "Episode : 77000, epsilon : 0.099\n",
      "Episode : 78000, epsilon : 0.096\n",
      "Episode : 79000, epsilon : 0.094\n",
      "Episode : 80000, epsilon : 0.091\n",
      "Episode : 81000, epsilon : 0.088\n",
      "Episode : 82000, epsilon : 0.086\n",
      "Episode : 83000, epsilon : 0.083\n",
      "Episode : 84000, epsilon : 0.081\n",
      "Episode : 85000, epsilon : 0.078\n",
      "Episode : 86000, epsilon : 0.076\n",
      "Episode : 87000, epsilon : 0.074\n",
      "Episode : 88000, epsilon : 0.071\n",
      "Episode : 89000, epsilon : 0.069\n",
      "Episode : 90000, epsilon : 0.067\n",
      "Episode : 91000, epsilon : 0.065\n",
      "Episode : 92000, epsilon : 0.063\n",
      "Episode : 93000, epsilon : 0.062\n",
      "Episode : 94000, epsilon : 0.060\n",
      "Episode : 95000, epsilon : 0.058\n",
      "Episode : 96000, epsilon : 0.056\n",
      "Episode : 97000, epsilon : 0.055\n",
      "Episode : 98000, epsilon : 0.053\n",
      "Episode : 99000, epsilon : 0.051\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, n_train=100000, n_pretrain=512, n_pretrain_ac=2000, n_learn=5, max_tau=50, decay_rate=2e-6, lr=5e-4, batch_size=256, log_dir='runs_dqn_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/otmane/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1 + np.exp(-4*(4e4 - 5e4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f65dced2950>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = np.linspace(0, 1e5, int(1e6))\n",
    "plt.plot(l, 1/(1 + np.exp(-(l - 5e4)/5e3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1, 1/(1 + np.exp(-4*(0 - 5e4)/1e3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
