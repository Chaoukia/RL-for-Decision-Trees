{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from nltk import Tree\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class represeting the one-hot encoder of the states.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Encoder.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        d          : Int, the input dimension.\n",
    "        dim        : Int, the one-hot encoded representation dimension\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.categories = categories\n",
    "        self.d = len(categories)\n",
    "        self.dim = np.sum(categories) + self.d\n",
    "        \n",
    "    def transform(self, state_values):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the vector state representation with dummies.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        state_values : List of length d where the ith entry is either NaN or the the feature value.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        state_one_hot : 2D np.array representing the one-hot encoded state.\n",
    "        \"\"\"\n",
    "    \n",
    "        s = 0\n",
    "        state_one_hot = np.zeros((1, self.dim), dtype = np.float32)\n",
    "        for i, value in enumerate(state_values):\n",
    "            if np.isnan(value):\n",
    "                state_one_hot[0, self.categories[i] + s] = 1\n",
    "\n",
    "            else:\n",
    "                state_one_hot[0, value + s] = 1\n",
    "\n",
    "            s += self.categories[i]+1\n",
    "\n",
    "        return state_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing a state, it also serves as Node representation\n",
    "    for our Breadth First Search\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, values, encoder, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class State.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        values     : List of length d (Input dimension):\n",
    "                         - values[i] = NaN if i is an unobsorved feature.\n",
    "                         - values[i] = value of feature i if it is observed.\n",
    "        encocer    : sklearn.preprocessing._encoders.OneHotEncoder object.\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        observed   : List containing the observed features at this state.\n",
    "        unobserved : List containing the unobserved features at this state.\n",
    "        empty      : Boolean, whether it is the empty state or not.\n",
    "        complete   : Boolean, whether all features are observed or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        d = len(values)\n",
    "        values_nans = np.isnan(values)\n",
    "        self.encoder = encoder  # One-hot encoder, used in the approximate RL framework for state representation.\n",
    "        self.values = values\n",
    "        self.values_encoded = self.encode()\n",
    "        self.categories = categories\n",
    "        self.observed = np.arange(d)[np.invert(values_nans)]\n",
    "        self.unobserved = np.arange(d)[values_nans]  # These are also the allowed query actions at this state\n",
    "        self.empty = (len(self.observed) == 0)\n",
    "        self.complete = (len(self.unobserved) == 0)\n",
    "        \n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the state with dummy variables. To be used when a one-hot encoder is defined.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        np.array of shape (1, #one_hot_representation_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.encoder.transform(self.values)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        # State initialization\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                # share in memory\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=25, out=8):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        Constructor of Deep Q-network class.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        input_size : Int, the one-hot encoding representation dimension.\n",
    "        out        : Int, output dimension, equal to the number of possible actions.\n",
    "        fc_1       : nn.Linear, first fully connected layer.\n",
    "        fc_2       : nn.Linear, second fully connected layer.\n",
    "        output     : nn.Linear, output fully connected layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_size, 32)\n",
    "        self.fc_2 = nn.Linear(32, 32)\n",
    "        \n",
    "        self.actor_output = nn.Linear(32, out)\n",
    "        self.critic_output = nn.Linear(32, 1)\n",
    "        \n",
    "        nn.init.constant_(self.actor_output.weight, 0)\n",
    "        nn.init.constant_(self.actor_output.bias, 0)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        The forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        x : torch.tensor of dimension (batch_size, input_size)\n",
    "        \n",
    "        Returns\n",
    "        ---------------\n",
    "        torch.tensor of dimension (batch_size, out)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        \n",
    "        return self.critic_output(x), self.actor_output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing the environment, it can generate data points that start each episode,\n",
    "    keep track of the current state, return the reward of an action taken at the current state,\n",
    "    and transition to the next corresponding state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, generator, rewards_queries, encoder, r_plus=5, r_minus=-5, split=3):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Environment.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        generator       : Dict, - keys   : Feature variables.\n",
    "                                - values : List of probability masses of each category of the corresponding feature.\n",
    "        rewards_queries : Dict, - keys   : Feature variables.\n",
    "                                - values : Reward of querying the value of the corresponding feature.\n",
    "        encocer         : sklearn.preprocessing._encoders.OneHotEncoder object.\n",
    "        r_plus          : Int, reward of a correct report (default=5).\n",
    "        r_minus         : Int, reward of an incorrect report (default=-5).\n",
    "        split           : Int, the split point we use to define our concept.\n",
    "        d               : Int, the number of feature variables.\n",
    "        data_point      : List of length d, the data point starting the episode.\n",
    "        label           : Boolean, the true label of data_point.\n",
    "        state           : Object of class State, the current state.\n",
    "        done            : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.generator = generator\n",
    "        self.categories = [len(v) for v in self.generator.values()]\n",
    "        self.d = len(self.categories)\n",
    "        self.rewards_queries = rewards_queries\n",
    "        self.encoder = encoder\n",
    "        self.r_plus = r_plus\n",
    "        self.r_minus = r_minus\n",
    "        self.split = split\n",
    "        \n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Generate a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        List with the values of each feature, it represents the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        return [np.random.choice(self.categories[i], p=self.generator[i]) for i in range(self.d)]\n",
    "    \n",
    "    def concept(self, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Define the concept labeling the data points. we can define it as a decision tree for example.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        Int in {0, 1}, the label of the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        label = True\n",
    "        i = 0\n",
    "        while label and i <= self.d-1:\n",
    "            if data_point[i] >= self.split:\n",
    "                label = False\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def reset(self, data_point=None):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Reset the environment to start a new episode. If data_point is specified, start the episode from it,\n",
    "        otherwise generate it.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label (default=None).\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_point = self.generate() if data_point is None else data_point\n",
    "        self.label = self.concept(self.data_point)\n",
    "        self.state = State([np.NaN for i in range(self.d)], self.encoder, categories=self.categories)\n",
    "        self.done = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Interract with the environment through an action taken at the current state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}, \n",
    "                 - 0, ..., d-1 represent query actions.\n",
    "                 - d, d+1 represent report actions.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        reward     : Int, the reward of taking this action at the current state.\n",
    "        next_state : Object of class State, the next state.\n",
    "        done       : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Treating query actions.\n",
    "        if action <= self.d-1:\n",
    "            # If it is an allowed query action.\n",
    "            if np.isnan(self.state.values[action]):\n",
    "                reward = self.rewards_queries[action]\n",
    "                values = self.state.values\n",
    "                values[action] = self.data_point[action] # Reveal the value of the queried feature in the data point.\n",
    "                self.state = State(values, self.encoder, self.categories)\n",
    "                \n",
    "            # If this query action is not allowed.\n",
    "            else:\n",
    "                print('unallowed')\n",
    "            \n",
    "        # Treating report actions.\n",
    "        else:\n",
    "            reward = self.r_plus if (action%self.d) == self.label else self.r_minus\n",
    "            self.done = True\n",
    "            \n",
    "        return reward, self.state, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(mp.Process):\n",
    "    def __init__(self, gnet, opt, global_ep, global_ep_r, res_queue, name):\n",
    "        super(Worker, self).__init__()\n",
    "        self.name = 'w%02i' % name\n",
    "        self.g_ep, self.g_ep_r, self.res_queue = global_ep, global_ep_r, res_queue\n",
    "        self.gnet, self.opt = gnet, opt\n",
    "        self.lnet = ActorCritic(input_size=np.sum(categories)+self.d, out=self.d+self.b)\n",
    "        self.env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "    def run(self):\n",
    "        total_step = 1\n",
    "        while self.g_ep.value < MAX_EP:\n",
    "            s = self.env.reset()\n",
    "            buffer_s, buffer_a, buffer_r = [], [], []\n",
    "            ep_r = 0.\n",
    "            while True:\n",
    "                if self.name == 'w00':\n",
    "                    self.env.render()\n",
    "                a = self.lnet.choose_action(v_wrap(s[None, :]))\n",
    "                s_, r, done, _ = self.env.step(a)\n",
    "                if done: r = -1\n",
    "                ep_r += r\n",
    "                buffer_a.append(a)\n",
    "                buffer_s.append(s)\n",
    "                buffer_r.append(r)\n",
    "\n",
    "                if total_step % UPDATE_GLOBAL_ITER == 0 or done:  # update global and assign to local net\n",
    "                    # sync\n",
    "                    push_and_pull(self.opt, self.lnet, self.gnet, done, s_, buffer_s, buffer_a, buffer_r, GAMMA)\n",
    "                    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "\n",
    "                    if done:  # done and print information\n",
    "                        record(self.g_ep, self.g_ep_r, ep_r, self.res_queue, self.name)\n",
    "                        break\n",
    "                s = s_\n",
    "                total_step += 1\n",
    "        self.res_queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class describing a DQN agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, generator, rewards_queries, encoder, name, global_network,\n",
    "                 optimizer, gamma=.9, categories=[4, 3, 3, 3, 2, 4], \n",
    "                 labels=[0, 1], min_queries=4, r_plus=5, r_minus=-10, split=4):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class AgentDQN.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        gamma           : Float in ]0, 1[, the discount factor (default=0.9).\n",
    "        categories      : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        labels          : List of the possible labels.\n",
    "        d               : Int, the number of feature variables.\n",
    "        b               : Int, the number of class labels.\n",
    "        actions         : List of all actions.\n",
    "        actions_queries : List of query actions.\n",
    "        actions_report  : List of report actions.\n",
    "\n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Worker, self).__init__()\n",
    "        self.name = 'w%02i' % name\n",
    "        self.env = Environment(generator, rewards_queries, encoder, r_plus=5, r_minus=-10, split=4)\n",
    "        self.global_network, self.optimizer = global_network, optimizer\n",
    "        self.gamma = gamma\n",
    "        self.categories = categories\n",
    "        self.labels = labels\n",
    "        self.min_queries = min_queries\n",
    "        self.d = len(categories)\n",
    "        self.b = len(labels)\n",
    "        self.actions = range(self.d + len(labels))\n",
    "        self.actions_queries = range(d)\n",
    "        self.actions_report = [self.d + label for label in labels]\n",
    "        self.local_network = ActorCritic(input_size=np.sum(categories)+self.d, out=self.d+self.b)\n",
    "        \n",
    "        \n",
    "    def actions_probas(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Calculate the probabilities of the allowed actions at a state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state   : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        actions_probas  : torch.tensor of size #Allowed_actions, with the allowed actions probabilities.\n",
    "        actions_allowed : List of the allowed actions.\n",
    "        \"\"\"\n",
    "        \n",
    "        value, actions_output = self.actor_critic(torch.from_numpy(state.values_encoded))\n",
    "        if len(state.observed) < self.min_queries:\n",
    "            actions_allowed = list(state.unobserved)\n",
    "            \n",
    "        else:\n",
    "            actions_allowed = list(state.unobserved) + [self.d+i for i in range(self.b)]\n",
    "            \n",
    "        actions_probas = F.softmax(actions_output[0, actions_allowed], dim=0)\n",
    "        return actions_probas, actions_allowed, value\n",
    "        \n",
    "    def action(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose an action at a state by sampling from the current stochastic policy.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state  : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        action, action_proba : Int in {0, ..., d-1, d, d+1}, Float in [0, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        actions_probas, actions_allowed, value = self.actions_probas(state)\n",
    "        m = Categorical(actions_probas)\n",
    "        index = m.sample()\n",
    "        action, action_log_prob = actions_allowed[index], m.log_prob(index)\n",
    "        return action, action_log_prob, actions_probas, value\n",
    "    \n",
    "    def action_greedy(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose the action maximizing the stochastic policy at the state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state  : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions_probas, actions_allowed, _ = self.actions_probas(state)\n",
    "            return actions_allowed[torch.argmax(actions_probas).item()]\n",
    "        \n",
    "        \n",
    "    def run(self, env, n_train=1000, lr=3e-4, log_dir='runs_actor_critic/', n_prints=1000, max_step=8, lambd=1e-3, clip_grad=.1):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Explore the environment and train the agent.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env      : Object of class Environment.\n",
    "        n_train  : Int, number of training episodes.\n",
    "        lr       : Float, the learning rate.\n",
    "        log_dir  : String, path of the folder where tensorboard events are stored.\n",
    "        n_prints : Int, the number of iterations between two consecutive prints.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "        \n",
    "        optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "#         label = 0\n",
    "        for episode in range(n_train):\n",
    "            env.reset()\n",
    "#             while env.label == label:\n",
    "#                 env.reset()\n",
    "                \n",
    "#             label = 1 - label\n",
    "            state = deepcopy(env.state)\n",
    "            rewards, values, log_probs = [], [], []\n",
    "            step = 0\n",
    "            entropy = 0\n",
    "            while (not env.done) and (step <= max_step):\n",
    "                action, action_log_prob, actions_probas, value = self.action(state)\n",
    "                reward, next_state, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                log_probs.append(action_log_prob.reshape(1, 1))\n",
    "                state = deepcopy(next_state)\n",
    "                entropy += -(actions_probas*torch.log(actions_probas)).sum()\n",
    "                step += 1\n",
    "                \n",
    "            R = torch.tensor(0, dtype=torch.float32) if env.done else self.actor_critic(torch.from_numpy(state.values_encoded))[0].detach()\n",
    "            size = len(values)\n",
    "            values_target = [0 for i in range(size)]\n",
    "            for t in range(size-1, -1, -1):\n",
    "                R = torch.tensor(rewards[t], dtype=torch.float32) + self.gamma*R\n",
    "                values_target[t] = R.reshape(1, 1)\n",
    "                \n",
    "            values_target = torch.cat(values_target)\n",
    "            values_current = torch.cat(values)\n",
    "            log_probs = torch.cat(log_probs)\n",
    "            \n",
    "            advantage = values_target - values_current\n",
    "            critic_loss = 0.5*(advantage**2).mean()\n",
    "            actor_loss = -(log_probs*advantage.detach()).mean()\n",
    "            entropy_loss = -entropy/size\n",
    "            loss = actor_loss + critic_loss + lambd*entropy_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor_critic.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "            \n",
    "            writer.add_scalar('Losses/Actor', actor_loss.item(), episode)\n",
    "            writer.add_scalar('Losses/Critic', critic_loss.item(), episode)\n",
    "            writer.add_scalar('Losses/Entropy', entropy_loss, episode)\n",
    "            writer.add_scalar('Losses/Loss', loss.item(), episode)\n",
    "            writer.add_scalar('Return', np.sum(rewards), episode)\n",
    "            if episode%n_prints == 0:\n",
    "                print('Episode : %d' %(episode))\n",
    "                        \n",
    "        writer.close()\n",
    "        \n",
    "    def predict(self, env, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Predict the label of a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        env.reset(data_point)\n",
    "        state = env.state\n",
    "        while not env.done:\n",
    "            action = agent.action_greedy(state)\n",
    "            env.step(action)\n",
    "            state = env.state\n",
    "        \n",
    "        return action%self.d\n",
    "        \n",
    "    def test(self, env, n_test=1000):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Test the agent on n_test data points generated by env.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env      : Object of class Environment.\n",
    "        n_test   : Int, number of data points to test the agent on.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        accuracy : FLoat in [0, 1], the accuracy of the agent on this test.\n",
    "        \"\"\"\n",
    "        \n",
    "        valids = 0\n",
    "        for i in range(n_test):\n",
    "            data_point = env.generate()\n",
    "            env.reset(data_point)\n",
    "            label_pred, label_true = self.predict(env, data_point), env.label\n",
    "            valids += (label_pred==label_true)\n",
    "            \n",
    "        return valids/n_test\n",
    "    \n",
    "    def save_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Save the agents q-network weights.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        torch.save(self.actor_critic.state_dict(), path)\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Load the weights of a q-network.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.policy.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def children(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Give the possible outcomes of taking the greedy policy at the considered state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        children : Set of objects of class State.\n",
    "        action   : Int, action taken at state with the agent policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        children = []\n",
    "        action = self.action_greedy(state)\n",
    "        if action >= self.d: return children, action\n",
    "        for category in range(self.categories[action]):\n",
    "            values = state.values.copy()\n",
    "            values[action] = category\n",
    "            children.append(State(values, state.encoder, self.categories))\n",
    "\n",
    "        return children, action\n",
    "    \n",
    "    def build_string_state(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision (with parentheses) starting from state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        l, action = self.children(state)\n",
    "        if action >= self.d: return str(self.action_greedy(state)%self.d) + ''\n",
    "        string = ''\n",
    "        for child in l:\n",
    "            string += '(X_' + str(action) + '=' + str(child.values[action]) + ' ' + self.build_string_state(child) + ') '\n",
    "\n",
    "        return string\n",
    "    \n",
    "    def build_string(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        return '( ' + self.build_string_state(State([np.NaN for i in range(self.d)], encoder, self.categories)) + ')'\n",
    "    \n",
    "    def plot_tree(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Plot the agent's decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        nltk tree object, helpful to visualize the agent's decision tree policy.\n",
    "        \"\"\"\n",
    "\n",
    "        return Tree.fromstring(self.build_string(encoder))\n",
    "        \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [3, 5, 3, 3, 3, 5, 3, 5, 3, 3, 4, 3, 3, 3, 5]\n",
    "labels = [0, 1]\n",
    "d, b = len(categories), len(labels)\n",
    "actions = range(d + len(labels))\n",
    "\n",
    "# Feature variables are independent and uniform\n",
    "generator = dict([(i, np.full(categories[i], 1/categories[i])) for i in range(len(categories))])\n",
    "\n",
    "# Each query action costs -1\n",
    "rewards_queries = dict([(i, -.5) for i in range(len(categories))])\n",
    "\n",
    "# Define ethe encoder\n",
    "encoder = Encoder(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(generator, rewards_queries, encoder, r_plus=5, r_minus=-10, split=4)\n",
    "agent = Agent(categories=categories, min_queries=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = []\n",
    "# for i in range(10000):\n",
    "#     data_point = env.generate()\n",
    "#     l.append(env.concept(data_point))\n",
    "\n",
    "# print('Fraction of 1 labeled points : %.5f' %(np.sum(l)/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0\n",
      "Episode : 1000\n",
      "Episode : 2000\n",
      "Episode : 3000\n",
      "Episode : 4000\n",
      "Episode : 5000\n",
      "Episode : 6000\n",
      "Episode : 7000\n",
      "Episode : 8000\n",
      "Episode : 9000\n",
      "Episode : 10000\n",
      "Episode : 11000\n",
      "Episode : 12000\n",
      "Episode : 13000\n",
      "Episode : 14000\n",
      "Episode : 15000\n",
      "Episode : 16000\n",
      "Episode : 17000\n",
      "Episode : 18000\n",
      "Episode : 19000\n",
      "Episode : 20000\n",
      "Episode : 21000\n",
      "Episode : 22000\n",
      "Episode : 23000\n",
      "Episode : 24000\n",
      "Episode : 25000\n",
      "Episode : 26000\n",
      "Episode : 27000\n",
      "Episode : 28000\n",
      "Episode : 29000\n",
      "Episode : 30000\n",
      "Episode : 31000\n",
      "Episode : 32000\n",
      "Episode : 33000\n",
      "Episode : 34000\n",
      "Episode : 35000\n",
      "Episode : 36000\n",
      "Episode : 37000\n",
      "Episode : 38000\n",
      "Episode : 39000\n",
      "Episode : 40000\n",
      "Episode : 41000\n",
      "Episode : 42000\n",
      "Episode : 43000\n",
      "Episode : 44000\n",
      "Episode : 45000\n",
      "Episode : 46000\n",
      "Episode : 47000\n",
      "Episode : 48000\n",
      "Episode : 49000\n",
      "Episode : 50000\n",
      "Episode : 51000\n",
      "Episode : 52000\n",
      "Episode : 53000\n",
      "Episode : 54000\n",
      "Episode : 55000\n",
      "Episode : 56000\n",
      "Episode : 57000\n",
      "Episode : 58000\n",
      "Episode : 59000\n",
      "Episode : 60000\n",
      "Episode : 61000\n",
      "Episode : 62000\n",
      "Episode : 63000\n",
      "Episode : 64000\n",
      "Episode : 65000\n",
      "Episode : 66000\n",
      "Episode : 67000\n",
      "Episode : 68000\n",
      "Episode : 69000\n",
      "Episode : 70000\n",
      "Episode : 71000\n",
      "Episode : 72000\n",
      "Episode : 73000\n",
      "Episode : 74000\n",
      "Episode : 75000\n",
      "Episode : 76000\n",
      "Episode : 77000\n",
      "Episode : 78000\n",
      "Episode : 79000\n",
      "Episode : 80000\n",
      "Episode : 81000\n",
      "Episode : 82000\n",
      "Episode : 83000\n",
      "Episode : 84000\n",
      "Episode : 85000\n",
      "Episode : 86000\n",
      "Episode : 87000\n",
      "Episode : 88000\n",
      "Episode : 89000\n",
      "Episode : 90000\n",
      "Episode : 91000\n",
      "Episode : 92000\n",
      "Episode : 93000\n",
      "Episode : 94000\n",
      "Episode : 95000\n",
      "Episode : 96000\n",
      "Episode : 97000\n",
      "Episode : 98000\n",
      "Episode : 99000\n",
      "Episode : 100000\n",
      "Episode : 101000\n",
      "Episode : 102000\n",
      "Episode : 103000\n",
      "Episode : 104000\n",
      "Episode : 105000\n",
      "Episode : 106000\n",
      "Episode : 107000\n",
      "Episode : 108000\n",
      "Episode : 109000\n",
      "Episode : 110000\n",
      "Episode : 111000\n",
      "Episode : 112000\n",
      "Episode : 113000\n",
      "Episode : 114000\n",
      "Episode : 115000\n",
      "Episode : 116000\n",
      "Episode : 117000\n",
      "Episode : 118000\n",
      "Episode : 119000\n",
      "Episode : 120000\n",
      "Episode : 121000\n",
      "Episode : 122000\n",
      "Episode : 123000\n",
      "Episode : 124000\n",
      "Episode : 125000\n",
      "Episode : 126000\n",
      "Episode : 127000\n",
      "Episode : 128000\n",
      "Episode : 129000\n",
      "Episode : 130000\n",
      "Episode : 131000\n",
      "Episode : 132000\n",
      "Episode : 133000\n",
      "Episode : 134000\n",
      "Episode : 135000\n",
      "Episode : 136000\n",
      "Episode : 137000\n",
      "Episode : 138000\n",
      "Episode : 139000\n",
      "Episode : 140000\n",
      "Episode : 141000\n",
      "Episode : 142000\n",
      "Episode : 143000\n",
      "Episode : 144000\n",
      "Episode : 145000\n",
      "Episode : 146000\n",
      "Episode : 147000\n",
      "Episode : 148000\n",
      "Episode : 149000\n",
      "Episode : 150000\n",
      "Episode : 151000\n",
      "Episode : 152000\n",
      "Episode : 153000\n",
      "Episode : 154000\n",
      "Episode : 155000\n",
      "Episode : 156000\n",
      "Episode : 157000\n",
      "Episode : 158000\n",
      "Episode : 159000\n",
      "Episode : 160000\n",
      "Episode : 161000\n",
      "Episode : 162000\n",
      "Episode : 163000\n",
      "Episode : 164000\n",
      "Episode : 165000\n",
      "Episode : 166000\n",
      "Episode : 167000\n",
      "Episode : 168000\n",
      "Episode : 169000\n",
      "Episode : 170000\n",
      "Episode : 171000\n",
      "Episode : 172000\n",
      "Episode : 173000\n",
      "Episode : 174000\n",
      "Episode : 175000\n",
      "Episode : 176000\n",
      "Episode : 177000\n",
      "Episode : 178000\n",
      "Episode : 179000\n",
      "Episode : 180000\n",
      "Episode : 181000\n",
      "Episode : 182000\n",
      "Episode : 183000\n",
      "Episode : 184000\n",
      "Episode : 185000\n",
      "Episode : 186000\n",
      "Episode : 187000\n",
      "Episode : 188000\n",
      "Episode : 189000\n",
      "Episode : 190000\n",
      "Episode : 191000\n",
      "Episode : 192000\n",
      "Episode : 193000\n",
      "Episode : 194000\n",
      "Episode : 195000\n",
      "Episode : 196000\n",
      "Episode : 197000\n",
      "Episode : 198000\n",
      "Episode : 199000\n",
      "Episode : 200000\n",
      "Episode : 201000\n",
      "Episode : 202000\n",
      "Episode : 203000\n",
      "Episode : 204000\n",
      "Episode : 205000\n",
      "Episode : 206000\n",
      "Episode : 207000\n",
      "Episode : 208000\n",
      "Episode : 209000\n",
      "Episode : 210000\n",
      "Episode : 211000\n",
      "Episode : 212000\n",
      "Episode : 213000\n",
      "Episode : 214000\n",
      "Episode : 215000\n",
      "Episode : 216000\n",
      "Episode : 217000\n",
      "Episode : 218000\n",
      "Episode : 219000\n",
      "Episode : 220000\n",
      "Episode : 221000\n",
      "Episode : 222000\n",
      "Episode : 223000\n",
      "Episode : 224000\n",
      "Episode : 225000\n",
      "Episode : 226000\n",
      "Episode : 227000\n",
      "Episode : 228000\n",
      "Episode : 229000\n",
      "Episode : 230000\n",
      "Episode : 231000\n",
      "Episode : 232000\n",
      "Episode : 233000\n",
      "Episode : 234000\n",
      "Episode : 235000\n",
      "Episode : 236000\n",
      "Episode : 237000\n",
      "Episode : 238000\n",
      "Episode : 239000\n",
      "Episode : 240000\n",
      "Episode : 241000\n",
      "Episode : 242000\n",
      "Episode : 243000\n",
      "Episode : 244000\n",
      "Episode : 245000\n",
      "Episode : 246000\n",
      "Episode : 247000\n",
      "Episode : 248000\n",
      "Episode : 249000\n",
      "Episode : 250000\n",
      "Episode : 251000\n",
      "Episode : 252000\n",
      "Episode : 253000\n",
      "Episode : 254000\n",
      "Episode : 255000\n",
      "Episode : 256000\n",
      "Episode : 257000\n",
      "Episode : 258000\n",
      "Episode : 259000\n",
      "Episode : 260000\n",
      "Episode : 261000\n",
      "Episode : 262000\n",
      "Episode : 263000\n",
      "Episode : 264000\n",
      "Episode : 265000\n",
      "Episode : 266000\n",
      "Episode : 267000\n",
      "Episode : 268000\n",
      "Episode : 269000\n",
      "Episode : 270000\n",
      "Episode : 271000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-82ef2fdec6af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'runs_actor_critic_freestyle/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-35b4a2a9c9f7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, n_train, lr, log_dir, n_prints, max_step, lambd, clip_grad)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues_target\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues_current\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvantage\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0madvantage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mentropy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(env, n_train=500000, lr=3e-4, lambd=1e-1, max_step=10, log_dir='runs_actor_critic_freestyle/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.load_weights('pg_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7728"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test(env, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred, labels_true = [], []\n",
    "for i in range(10000):\n",
    "    data_point = env.generate()\n",
    "    labels_pred.append(agent.predict(env, data_point))\n",
    "    labels_true.append(env.concept(data_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true ones      : 4048\n",
      "Number of predicted ones : 6410\n"
     ]
    }
   ],
   "source": [
    "print('Number of true ones      : %d' %np.sum(labels_true))\n",
    "print('Number of predicted ones : %d' %np.sum(labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for i in range(1000):\n",
    "    data_point = env.generate()\n",
    "    env.reset(data_point)\n",
    "    state = env.state\n",
    "    depth = 0\n",
    "    while not env.done:\n",
    "        action = agent.action_greedy(state)\n",
    "        _, state, _ = env.step(action)\n",
    "        depth += 1\n",
    "        \n",
    "    depths.append(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdb01b9ffd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(depths, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.7032]], grad_fn=<AddmmBackward>),\n",
       " tensor([[0.6417, 1.0848, 0.4278, 0.3099, 0.5529, 1.0024, 0.3818, 1.2002, 0.4065,\n",
       "          0.2040, 0.1194, 0.6580, 0.2598, 0.4405, 1.0552, 7.4219, 7.8306]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_critic(torch.from_numpy(State([np.NaN for i in range(d)], encoder).values_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.4159]], grad_fn=<AddmmBackward>),\n",
       " tensor([[0.7261, 1.1249, 1.1620, 0.6731, 1.1138, 0.5871, 0.9981, 0.8198, 1.3080,\n",
       "          0.9367, 0.4809, 0.9330, 1.0470, 0.8875, 0.9782, 2.4003, 4.4774]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_critic(torch.from_numpy(State([np.NaN, 1, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, \n",
    "                                           0, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN], encoder).values_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 3, 3, 3, 5, 3, 5, 3, 3, 4, 3, 3, 3, 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('', [Tree('X_7=0', [Tree('X_1=0', ['1']), Tree('X_1=1', ['1']), Tree('X_1=2', ['1']), Tree('X_1=3', ['1']), Tree('X_1=4', ['0'])]), Tree('X_7=1', [Tree('X_1=0', ['1']), Tree('X_1=1', ['1']), Tree('X_1=2', ['1']), Tree('X_1=3', ['1']), Tree('X_1=4', ['0'])]), Tree('X_7=2', [Tree('X_1=0', ['1']), Tree('X_1=1', ['1']), Tree('X_1=2', ['1']), Tree('X_1=3', ['1']), Tree('X_1=4', ['0'])]), Tree('X_7=3', [Tree('X_1=0', ['1']), Tree('X_1=1', ['1']), Tree('X_1=2', ['1']), Tree('X_1=3', ['1']), Tree('X_1=4', ['0'])]), Tree('X_7=4', [Tree('X_0=0', ['0']), Tree('X_0=1', ['0']), Tree('X_0=2', ['0'])])])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.plot_tree(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
