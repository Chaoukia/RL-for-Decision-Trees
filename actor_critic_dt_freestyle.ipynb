{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from nltk import Tree\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class represeting the one-hot encoder of the states.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Encoder.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        d          : Int, the input dimension.\n",
    "        dim        : Int, the one-hot encoded representation dimension\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.categories = categories\n",
    "        self.d = len(categories)\n",
    "        self.dim = np.sum(categories) + self.d\n",
    "        \n",
    "    def transform(self, state_values):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the vector state representation with dummies.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        state_values : List of length d where the ith entry is either NaN or the the feature value.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        state_one_hot : 2D np.array representing the one-hot encoded state.\n",
    "        \"\"\"\n",
    "    \n",
    "        s = 0\n",
    "        state_one_hot = np.zeros((1, self.dim), dtype = np.float32)\n",
    "        for i, value in enumerate(state_values):\n",
    "            if np.isnan(value):\n",
    "                state_one_hot[0, self.categories[i] + s] = 1\n",
    "\n",
    "            else:\n",
    "                state_one_hot[0, value + s] = 1\n",
    "\n",
    "            s += self.categories[i]+1\n",
    "\n",
    "        return state_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing a state, it also serves as Node representation\n",
    "    for our Breadth First Search\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, values, encoder, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class State.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        values     : List of length d (Input dimension):\n",
    "                         - values[i] = NaN if i is an unobsorved feature.\n",
    "                         - values[i] = value of feature i if it is observed.\n",
    "        encocer    : sklearn.preprocessing._encoders.OneHotEncoder object.\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        observed   : List containing the observed features at this state.\n",
    "        unobserved : List containing the unobserved features at this state.\n",
    "        empty      : Boolean, whether it is the empty state or not.\n",
    "        complete   : Boolean, whether all features are observed or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        d = len(values)\n",
    "        values_nans = np.isnan(values)\n",
    "        self.encoder = encoder  # One-hot encoder, used in the approximate RL framework for state representation.\n",
    "        self.values = values\n",
    "        self.values_encoded = self.encode()\n",
    "        self.categories = categories\n",
    "        self.observed = np.arange(d)[np.invert(values_nans)]\n",
    "        self.unobserved = np.arange(d)[values_nans]  # These are also the allowed query actions at this state\n",
    "        self.empty = (len(self.observed) == 0)\n",
    "        self.complete = (len(self.unobserved) == 0)\n",
    "        \n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the state with dummy variables. To be used when a one-hot encoder is defined.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        np.array of shape (1, #one_hot_representation_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.encoder.transform(self.values)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=25, out=8):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        Constructor of Deep Q-network class.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        input_size : Int, the one-hot encoding representation dimension.\n",
    "        out        : Int, output dimension, equal to the number of possible actions.\n",
    "        fc_1       : nn.Linear, first fully connected layer.\n",
    "        fc_2       : nn.Linear, second fully connected layer.\n",
    "        output     : nn.Linear, output fully connected layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_size, 32)\n",
    "        self.fc_2 = nn.Linear(32, 32)\n",
    "        \n",
    "        self.actor_output = nn.Linear(32, out)\n",
    "        self.critic_output = nn.Linear(32, 1)\n",
    "        \n",
    "        nn.init.constant_(self.actor_output.weight, 0)\n",
    "        nn.init.constant_(self.actor_output.bias, 0)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        The forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        x : torch.tensor of dimension (batch_size, input_size)\n",
    "        \n",
    "        Returns\n",
    "        ---------------\n",
    "        torch.tensor of dimension (batch_size, out)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        \n",
    "        return self.critic_output(x), self.actor_output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing the environment, it can generate data points that start each episode,\n",
    "    keep track of the current state, return the reward of an action taken at the current state,\n",
    "    and transition to the next corresponding state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, generator, rewards_queries, encoder, r_plus=5, r_minus=-5, split=3):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Environment.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        generator       : Dict, - keys   : Feature variables.\n",
    "                                - values : List of probability masses of each category of the corresponding feature.\n",
    "        rewards_queries : Dict, - keys   : Feature variables.\n",
    "                                - values : Reward of querying the value of the corresponding feature.\n",
    "        encocer         : sklearn.preprocessing._encoders.OneHotEncoder object.\n",
    "        r_plus          : Int, reward of a correct report (default=5).\n",
    "        r_minus         : Int, reward of an incorrect report (default=-5).\n",
    "        split           : Int, the split point we use to define our concept.\n",
    "        d               : Int, the number of feature variables.\n",
    "        data_point      : List of length d, the data point starting the episode.\n",
    "        label           : Boolean, the true label of data_point.\n",
    "        state           : Object of class State, the current state.\n",
    "        done            : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.generator = generator\n",
    "        self.categories = [len(v) for v in self.generator.values()]\n",
    "        self.d = len(self.categories)\n",
    "        self.rewards_queries = rewards_queries\n",
    "        self.encoder = encoder\n",
    "        self.r_plus = r_plus\n",
    "        self.r_minus = r_minus\n",
    "        self.split = split\n",
    "        \n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Generate a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        List with the values of each feature, it represents the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        return [np.random.choice(self.categories[i], p=self.generator[i]) for i in range(self.d)]\n",
    "    \n",
    "    def concept(self, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Define the concept labeling the data points. we can define it as a decision tree for example.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        Int in {0, 1}, the label of the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        label = True\n",
    "        i = 0\n",
    "        while label and i <= self.d-1:\n",
    "            if data_point[i] >= self.split:\n",
    "                label = False\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def reset(self, data_point=None):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Reset the environment to start a new episode. If data_point is specified, start the episode from it,\n",
    "        otherwise generate it.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label (default=None).\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_point = self.generate() if data_point is None else data_point\n",
    "        self.label = self.concept(self.data_point)\n",
    "        self.state = State([np.NaN for i in range(self.d)], self.encoder, categories=self.categories)\n",
    "        self.done = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Interract with the environment through an action taken at the current state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}, \n",
    "                 - 0, ..., d-1 represent query actions.\n",
    "                 - d, d+1 represent report actions.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        reward     : Int, the reward of taking this action at the current state.\n",
    "        next_state : Object of class State, the next state.\n",
    "        done       : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Treating query actions.\n",
    "        if action <= self.d-1:\n",
    "            # If it is an allowed query action.\n",
    "            if np.isnan(self.state.values[action]):\n",
    "                reward = self.rewards_queries[action]\n",
    "                values = self.state.values\n",
    "                values[action] = self.data_point[action] # Reveal the value of the queried feature in the data point.\n",
    "                self.state = State(values, self.encoder, self.categories)\n",
    "                \n",
    "            # If this query action is not allowed.\n",
    "            else:\n",
    "                print('unallowed')\n",
    "            \n",
    "        # Treating report actions.\n",
    "        else:\n",
    "            reward = self.r_plus if (action%self.d) == self.label else self.r_minus\n",
    "            self.done = True\n",
    "            \n",
    "        return reward, self.state, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class describing a DQN agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=.9, categories=[4, 3, 3, 3, 2, 4], labels=[0, 1], min_queries=4):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class AgentDQN.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        gamma           : Float in ]0, 1[, the discount factor (default=0.9).\n",
    "        categories      : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        labels          : List of the possible labels.\n",
    "        d               : Int, the number of feature variables.\n",
    "        b               : Int, the number of class labels.\n",
    "        actions         : List of all actions.\n",
    "        actions_queries : List of query actions.\n",
    "        actions_report  : List of report actions.\n",
    "\n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.categories = categories\n",
    "        self.labels = labels\n",
    "        self.min_queries = min_queries\n",
    "        self.d = len(categories)\n",
    "        self.b = len(labels)\n",
    "        self.actions = range(self.d + len(labels))\n",
    "        self.actions_queries = range(d)\n",
    "        self.actions_report = [self.d + label for label in labels]\n",
    "        self.actor_critic = ActorCritic(input_size=np.sum(categories)+self.d, out=self.d+self.b)\n",
    "        \n",
    "    def actions_probas(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Calculate the probabilities of the allowed actions at a state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state   : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        actions_probas  : torch.tensor of size #Allowed_actions, with the allowed actions probabilities.\n",
    "        actions_allowed : List of the allowed actions.\n",
    "        \"\"\"\n",
    "        \n",
    "        value, actions_output = self.actor_critic(torch.from_numpy(state.values_encoded))\n",
    "        if len(state.observed) < self.min_queries:\n",
    "            actions_allowed = list(state.unobserved)\n",
    "            \n",
    "        else:\n",
    "            actions_allowed = list(state.unobserved) + [self.d+i for i in range(self.b)]\n",
    "            \n",
    "        actions_probas = F.softmax(actions_output[0, actions_allowed], dim=0)\n",
    "        return actions_probas, actions_allowed, value\n",
    "        \n",
    "    def action(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose an action at a state by sampling from the current stochastic policy.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state  : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        action, action_proba : Int in {0, ..., d-1, d, d+1}, Float in [0, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        actions_probas, actions_allowed, value = self.actions_probas(state)\n",
    "        m = Categorical(actions_probas)\n",
    "        index = m.sample()\n",
    "        action, action_log_prob = actions_allowed[index], m.log_prob(index)\n",
    "        return action, action_log_prob, actions_probas, value\n",
    "    \n",
    "    def action_greedy(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose the action maximizing the stochastic policy at the state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state  : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions_probas, actions_allowed, _ = self.actions_probas(state)\n",
    "            return actions_allowed[torch.argmax(actions_probas).item()]\n",
    "        \n",
    "        \n",
    "    def train(self, env, n_train=1000, lr=3e-4, log_dir='runs_actor_critic/', n_prints=1000, max_step=8, lambd=1e-3, clip_grad=.1):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Explore the environment and train the agent.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env      : Object of class Environment.\n",
    "        n_train  : Int, number of training episodes.\n",
    "        lr       : Float, the learning rate.\n",
    "        log_dir  : String, path of the folder where tensorboard events are stored.\n",
    "        n_prints : Int, the number of iterations between two consecutive prints.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "        \n",
    "        optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "#         label = 0\n",
    "        it = 0\n",
    "        for episode in range(n_train):\n",
    "            env.reset()\n",
    "#             while env.label == label:\n",
    "#                 env.reset()\n",
    "                \n",
    "#             label = 1 - label\n",
    "            state = deepcopy(env.state)\n",
    "            episode_len = 0\n",
    "            episode_rewards = 0\n",
    "            while not env.done:\n",
    "                rewards, values, log_probs = [], [], []\n",
    "                step = 0\n",
    "                entropy = 0\n",
    "                while (not env.done) and (step <= max_step):\n",
    "                    action, action_log_prob, actions_probas, value = self.action(state)\n",
    "                    reward, next_state, _ = env.step(action)\n",
    "                    episode_rewards += reward\n",
    "                    rewards.append(reward)\n",
    "                    values.append(value)\n",
    "                    log_probs.append(action_log_prob.reshape(1, 1))\n",
    "                    state = deepcopy(next_state)\n",
    "                    entropy += -(actions_probas*torch.log(actions_probas)).sum()\n",
    "                    step += 1\n",
    "                    it += 1\n",
    "                    episode_len += 1\n",
    "\n",
    "                R = torch.tensor(0, dtype=torch.float32) if env.done else self.actor_critic(torch.from_numpy(state.values_encoded))[0].detach()\n",
    "                size = len(values)\n",
    "                values_target = [0 for i in range(size)]\n",
    "                for t in range(size-1, -1, -1):\n",
    "                    R = torch.tensor(rewards[t], dtype=torch.float32) + self.gamma*R\n",
    "                    values_target[t] = R.reshape(1, 1)\n",
    "\n",
    "                values_target = torch.cat(values_target)\n",
    "                values_current = torch.cat(values)\n",
    "                log_probs = torch.cat(log_probs)\n",
    "\n",
    "                advantage = values_target - values_current\n",
    "                critic_loss = 0.5*(advantage**2).mean()\n",
    "                actor_loss = -(log_probs*advantage.detach()).mean()\n",
    "                entropy_loss = -entropy/size\n",
    "                loss = actor_loss + critic_loss + lambd*entropy_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.actor_critic.parameters(), clip_grad)\n",
    "                optimizer.step()\n",
    "\n",
    "                writer.add_scalar('Losses/Actor', actor_loss.item(), it)\n",
    "                writer.add_scalar('Losses/Critic', critic_loss.item(), it)\n",
    "                writer.add_scalar('Losses/Entropy', entropy_loss, it)\n",
    "                writer.add_scalar('Losses/Loss', loss.item(), it)\n",
    "                \n",
    "            writer.add_scalar('Episode/Return', episode_rewards, episode)\n",
    "            writer.add_scalar('Episode/Length', episode_len, episode)\n",
    "            if episode%n_prints == 0:\n",
    "                print('Episode : %d' %(episode))\n",
    "                        \n",
    "        writer.close()\n",
    "        \n",
    "    def predict(self, env, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Predict the label of a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        env.reset(data_point)\n",
    "        state = env.state\n",
    "        while not env.done:\n",
    "            action = agent.action_greedy(state)\n",
    "            env.step(action)\n",
    "            state = env.state\n",
    "        \n",
    "        return action%self.d\n",
    "        \n",
    "    def test(self, env, n_test=1000):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Test the agent on n_test data points generated by env.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env      : Object of class Environment.\n",
    "        n_test   : Int, number of data points to test the agent on.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        accuracy : FLoat in [0, 1], the accuracy of the agent on this test.\n",
    "        \"\"\"\n",
    "        \n",
    "        valids = 0\n",
    "        for i in range(n_test):\n",
    "            data_point = env.generate()\n",
    "            env.reset(data_point)\n",
    "            label_pred, label_true = self.predict(env, data_point), env.label\n",
    "            valids += (label_pred==label_true)\n",
    "            \n",
    "        return valids/n_test\n",
    "    \n",
    "    def save_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Save the agents q-network weights.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        torch.save(self.actor_critic.state_dict(), path)\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Load the weights of a q-network.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.policy.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def children(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Give the possible outcomes of taking the greedy policy at the considered state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        children : Set of objects of class State.\n",
    "        action   : Int, action taken at state with the agent policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        children = []\n",
    "        action = self.action_greedy(state)\n",
    "        if action >= self.d: return children, action\n",
    "        for category in range(self.categories[action]):\n",
    "            values = state.values.copy()\n",
    "            values[action] = category\n",
    "            children.append(State(values, state.encoder, self.categories))\n",
    "\n",
    "        return children, action\n",
    "    \n",
    "    def build_string_state(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision (with parentheses) starting from state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        l, action = self.children(state)\n",
    "        if action >= self.d: return str(self.action_greedy(state)%self.d) + ''\n",
    "        string = ''\n",
    "        for child in l:\n",
    "            string += '(X_' + str(action) + '=' + str(child.values[action]) + ' ' + self.build_string_state(child) + ') '\n",
    "\n",
    "        return string\n",
    "    \n",
    "    def build_string(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        return '( ' + self.build_string_state(State([np.NaN for i in range(self.d)], encoder, self.categories)) + ')'\n",
    "    \n",
    "    def plot_tree(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Plot the agent's decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        nltk tree object, helpful to visualize the agent's decision tree policy.\n",
    "        \"\"\"\n",
    "\n",
    "        return Tree.fromstring(self.build_string(encoder))\n",
    "        \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [3, 5, 3, 3, 3, 5, 3, 5, 3, 3, 4, 3, 3, 3, 5]\n",
    "labels = [0, 1]\n",
    "d, b = len(categories), len(labels)\n",
    "actions = range(d + len(labels))\n",
    "\n",
    "# Feature variables are independent and uniform\n",
    "generator = dict([(i, np.full(categories[i], 1/categories[i])) for i in range(len(categories))])\n",
    "\n",
    "# Each query action costs -1\n",
    "rewards_queries = dict([(i, -.5) for i in range(len(categories))])\n",
    "\n",
    "# Define ethe encoder\n",
    "encoder = Encoder(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(generator, rewards_queries, encoder, r_plus=5, r_minus=-10, split=4)\n",
    "agent = Agent(categories=categories, min_queries=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = []\n",
    "# for i in range(10000):\n",
    "#     data_point = env.generate()\n",
    "#     l.append(env.concept(data_point))\n",
    "\n",
    "# print('Fraction of 1 labeled points : %.5f' %(np.sum(l)/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0\n",
      "Episode : 1000\n",
      "Episode : 2000\n",
      "Episode : 3000\n",
      "Episode : 4000\n",
      "Episode : 5000\n",
      "Episode : 6000\n",
      "Episode : 7000\n",
      "Episode : 8000\n",
      "Episode : 9000\n",
      "Episode : 10000\n",
      "Episode : 11000\n",
      "Episode : 12000\n",
      "Episode : 13000\n",
      "Episode : 14000\n",
      "Episode : 15000\n",
      "Episode : 16000\n",
      "Episode : 17000\n",
      "Episode : 18000\n",
      "Episode : 19000\n",
      "Episode : 20000\n",
      "Episode : 21000\n",
      "Episode : 22000\n",
      "Episode : 23000\n",
      "Episode : 24000\n",
      "Episode : 25000\n",
      "Episode : 26000\n",
      "Episode : 27000\n",
      "Episode : 28000\n",
      "Episode : 29000\n",
      "Episode : 30000\n",
      "Episode : 31000\n",
      "Episode : 32000\n",
      "Episode : 33000\n",
      "Episode : 34000\n",
      "Episode : 35000\n",
      "Episode : 36000\n",
      "Episode : 37000\n",
      "Episode : 38000\n",
      "Episode : 39000\n",
      "Episode : 40000\n",
      "Episode : 41000\n",
      "Episode : 42000\n",
      "Episode : 43000\n",
      "Episode : 44000\n",
      "Episode : 45000\n",
      "Episode : 46000\n",
      "Episode : 47000\n",
      "Episode : 48000\n",
      "Episode : 49000\n",
      "Episode : 50000\n",
      "Episode : 51000\n",
      "Episode : 52000\n",
      "Episode : 53000\n",
      "Episode : 54000\n",
      "Episode : 55000\n",
      "Episode : 56000\n",
      "Episode : 57000\n",
      "Episode : 58000\n",
      "Episode : 59000\n",
      "Episode : 60000\n",
      "Episode : 61000\n",
      "Episode : 62000\n",
      "Episode : 63000\n",
      "Episode : 64000\n",
      "Episode : 65000\n",
      "Episode : 66000\n",
      "Episode : 67000\n",
      "Episode : 68000\n",
      "Episode : 69000\n",
      "Episode : 70000\n",
      "Episode : 71000\n",
      "Episode : 72000\n",
      "Episode : 73000\n",
      "Episode : 74000\n",
      "Episode : 75000\n",
      "Episode : 76000\n",
      "Episode : 77000\n",
      "Episode : 78000\n",
      "Episode : 79000\n",
      "Episode : 80000\n",
      "Episode : 81000\n",
      "Episode : 82000\n",
      "Episode : 83000\n",
      "Episode : 84000\n",
      "Episode : 85000\n",
      "Episode : 86000\n",
      "Episode : 87000\n",
      "Episode : 88000\n",
      "Episode : 89000\n",
      "Episode : 90000\n",
      "Episode : 91000\n",
      "Episode : 92000\n",
      "Episode : 93000\n",
      "Episode : 94000\n",
      "Episode : 95000\n",
      "Episode : 96000\n",
      "Episode : 97000\n",
      "Episode : 98000\n",
      "Episode : 99000\n",
      "Episode : 100000\n",
      "Episode : 101000\n",
      "Episode : 102000\n",
      "Episode : 103000\n",
      "Episode : 104000\n",
      "Episode : 105000\n",
      "Episode : 106000\n",
      "Episode : 107000\n",
      "Episode : 108000\n",
      "Episode : 109000\n",
      "Episode : 110000\n",
      "Episode : 111000\n",
      "Episode : 112000\n",
      "Episode : 113000\n",
      "Episode : 114000\n",
      "Episode : 115000\n",
      "Episode : 116000\n",
      "Episode : 117000\n",
      "Episode : 118000\n",
      "Episode : 119000\n",
      "Episode : 120000\n",
      "Episode : 121000\n",
      "Episode : 122000\n",
      "Episode : 123000\n",
      "Episode : 124000\n",
      "Episode : 125000\n",
      "Episode : 126000\n",
      "Episode : 127000\n",
      "Episode : 128000\n",
      "Episode : 129000\n",
      "Episode : 130000\n",
      "Episode : 131000\n",
      "Episode : 132000\n",
      "Episode : 133000\n",
      "Episode : 134000\n",
      "Episode : 135000\n",
      "Episode : 136000\n",
      "Episode : 137000\n",
      "Episode : 138000\n",
      "Episode : 139000\n",
      "Episode : 140000\n",
      "Episode : 141000\n",
      "Episode : 142000\n",
      "Episode : 143000\n",
      "Episode : 144000\n",
      "Episode : 145000\n",
      "Episode : 146000\n",
      "Episode : 147000\n",
      "Episode : 148000\n",
      "Episode : 149000\n",
      "Episode : 150000\n",
      "Episode : 151000\n",
      "Episode : 152000\n",
      "Episode : 153000\n",
      "Episode : 154000\n",
      "Episode : 155000\n",
      "Episode : 156000\n",
      "Episode : 157000\n",
      "Episode : 158000\n",
      "Episode : 159000\n",
      "Episode : 160000\n",
      "Episode : 161000\n",
      "Episode : 162000\n",
      "Episode : 163000\n",
      "Episode : 164000\n",
      "Episode : 165000\n",
      "Episode : 166000\n",
      "Episode : 167000\n",
      "Episode : 168000\n",
      "Episode : 169000\n",
      "Episode : 170000\n",
      "Episode : 171000\n",
      "Episode : 172000\n",
      "Episode : 173000\n",
      "Episode : 174000\n",
      "Episode : 175000\n",
      "Episode : 176000\n",
      "Episode : 177000\n",
      "Episode : 178000\n",
      "Episode : 179000\n",
      "Episode : 180000\n",
      "Episode : 181000\n",
      "Episode : 182000\n",
      "Episode : 183000\n",
      "Episode : 184000\n",
      "Episode : 185000\n",
      "Episode : 186000\n",
      "Episode : 187000\n",
      "Episode : 188000\n",
      "Episode : 189000\n",
      "Episode : 190000\n",
      "Episode : 191000\n",
      "Episode : 192000\n",
      "Episode : 193000\n",
      "Episode : 194000\n",
      "Episode : 195000\n",
      "Episode : 196000\n",
      "Episode : 197000\n",
      "Episode : 198000\n",
      "Episode : 199000\n",
      "Episode : 200000\n",
      "Episode : 201000\n",
      "Episode : 202000\n",
      "Episode : 203000\n",
      "Episode : 204000\n",
      "Episode : 205000\n",
      "Episode : 206000\n",
      "Episode : 207000\n",
      "Episode : 208000\n",
      "Episode : 209000\n",
      "Episode : 210000\n",
      "Episode : 211000\n",
      "Episode : 212000\n",
      "Episode : 213000\n",
      "Episode : 214000\n",
      "Episode : 215000\n",
      "Episode : 216000\n",
      "Episode : 217000\n",
      "Episode : 218000\n",
      "Episode : 219000\n",
      "Episode : 220000\n",
      "Episode : 221000\n",
      "Episode : 222000\n",
      "Episode : 223000\n",
      "Episode : 224000\n",
      "Episode : 225000\n",
      "Episode : 226000\n",
      "Episode : 227000\n",
      "Episode : 228000\n",
      "Episode : 229000\n",
      "Episode : 230000\n",
      "Episode : 231000\n",
      "Episode : 232000\n",
      "Episode : 233000\n",
      "Episode : 234000\n",
      "Episode : 235000\n",
      "Episode : 236000\n",
      "Episode : 237000\n",
      "Episode : 238000\n",
      "Episode : 239000\n",
      "Episode : 240000\n",
      "Episode : 241000\n",
      "Episode : 242000\n",
      "Episode : 243000\n",
      "Episode : 244000\n",
      "Episode : 245000\n",
      "Episode : 246000\n",
      "Episode : 247000\n",
      "Episode : 248000\n",
      "Episode : 249000\n",
      "Episode : 250000\n",
      "Episode : 251000\n",
      "Episode : 252000\n",
      "Episode : 253000\n",
      "Episode : 254000\n",
      "Episode : 255000\n",
      "Episode : 256000\n",
      "Episode : 257000\n",
      "Episode : 258000\n",
      "Episode : 259000\n",
      "Episode : 260000\n",
      "Episode : 261000\n",
      "Episode : 262000\n",
      "Episode : 263000\n",
      "Episode : 264000\n",
      "Episode : 265000\n",
      "Episode : 266000\n",
      "Episode : 267000\n",
      "Episode : 268000\n",
      "Episode : 269000\n",
      "Episode : 270000\n",
      "Episode : 271000\n",
      "Episode : 272000\n",
      "Episode : 273000\n",
      "Episode : 274000\n",
      "Episode : 275000\n",
      "Episode : 276000\n",
      "Episode : 277000\n",
      "Episode : 278000\n",
      "Episode : 279000\n",
      "Episode : 280000\n",
      "Episode : 281000\n",
      "Episode : 282000\n",
      "Episode : 283000\n",
      "Episode : 284000\n",
      "Episode : 285000\n",
      "Episode : 286000\n",
      "Episode : 287000\n",
      "Episode : 288000\n",
      "Episode : 289000\n",
      "Episode : 290000\n",
      "Episode : 291000\n",
      "Episode : 292000\n",
      "Episode : 293000\n",
      "Episode : 294000\n",
      "Episode : 295000\n",
      "Episode : 296000\n",
      "Episode : 297000\n",
      "Episode : 298000\n",
      "Episode : 299000\n",
      "Episode : 300000\n",
      "Episode : 301000\n",
      "Episode : 302000\n",
      "Episode : 303000\n",
      "Episode : 304000\n",
      "Episode : 305000\n",
      "Episode : 306000\n",
      "Episode : 307000\n",
      "Episode : 308000\n",
      "Episode : 309000\n",
      "Episode : 310000\n",
      "Episode : 311000\n",
      "Episode : 312000\n",
      "Episode : 313000\n",
      "Episode : 314000\n",
      "Episode : 315000\n",
      "Episode : 316000\n",
      "Episode : 317000\n",
      "Episode : 318000\n",
      "Episode : 319000\n",
      "Episode : 320000\n",
      "Episode : 321000\n",
      "Episode : 322000\n",
      "Episode : 323000\n",
      "Episode : 324000\n",
      "Episode : 325000\n",
      "Episode : 326000\n",
      "Episode : 327000\n",
      "Episode : 328000\n",
      "Episode : 329000\n",
      "Episode : 330000\n",
      "Episode : 331000\n",
      "Episode : 332000\n",
      "Episode : 333000\n",
      "Episode : 334000\n",
      "Episode : 335000\n",
      "Episode : 336000\n",
      "Episode : 337000\n",
      "Episode : 338000\n",
      "Episode : 339000\n",
      "Episode : 340000\n",
      "Episode : 341000\n",
      "Episode : 342000\n",
      "Episode : 343000\n",
      "Episode : 344000\n",
      "Episode : 345000\n",
      "Episode : 346000\n",
      "Episode : 347000\n",
      "Episode : 348000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-096c1819f7df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'runs_long_reinforce_min_queries_2/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-8d6d066110d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, n_train, lr, log_dir, n_prints, max_step, lambd, clip_grad)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mentropy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mentropy_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(env, n_train=500000, lr=3e-4, lambd=1, max_step=30, log_dir='runs_long_reinforce_min_queries_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.load_weights('pg_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7698"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test(env, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred, labels_true = [], []\n",
    "for i in range(10000):\n",
    "    data_point = env.generate()\n",
    "    labels_pred.append(agent.predict(env, data_point))\n",
    "    labels_true.append(env.concept(data_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true ones      : 4043\n",
      "Number of predicted ones : 6387\n"
     ]
    }
   ],
   "source": [
    "print('Number of true ones      : %d' %np.sum(labels_true))\n",
    "print('Number of predicted ones : %d' %np.sum(labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for i in range(1000):\n",
    "    data_point = env.generate()\n",
    "    env.reset(data_point)\n",
    "    state = env.state\n",
    "    depth = 0\n",
    "    while not env.done:\n",
    "        action = agent.action_greedy(state)\n",
    "        _, state, _ = env.step(action)\n",
    "        depth += 1\n",
    "        \n",
    "    depths.append(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c18b2ef10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(depths, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4238]], grad_fn=<AddmmBackward>),\n",
       " tensor([[-0.0505,  0.4017,  0.0119, -0.1016, -0.0646,  0.4320,  0.0245,  0.4170,\n",
       "          -0.0943, -0.0072, -0.0691, -0.0294, -0.0926,  0.0232,  0.4071,  0.6783,\n",
       "          -0.5362]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_critic(torch.from_numpy(State([np.NaN for i in range(d)], encoder).values_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.6044]], grad_fn=<AddmmBackward>),\n",
       " tensor([[ 0.2150,  0.5515,  0.1956,  0.0539,  0.1084,  0.5920,  0.1994,  0.6596,\n",
       "           0.0110,  0.2298,  0.1498,  0.1570,  0.1220,  0.2210,  0.7321, -0.3781,\n",
       "           1.2719]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_critic(torch.from_numpy(State([np.NaN, 0, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, \n",
    "                                           0, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 3], encoder).values_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 3, 3, 3, 5, 3, 5, 3, 3, 4, 3, 3, 3, 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('', [Tree('X_5=0', [Tree('X_7=0', ['1']), Tree('X_7=1', ['1']), Tree('X_7=2', ['1']), Tree('X_7=3', ['1']), Tree('X_7=4', ['0'])]), Tree('X_5=1', [Tree('X_7=0', ['1']), Tree('X_7=1', ['1']), Tree('X_7=2', ['1']), Tree('X_7=3', ['1']), Tree('X_7=4', ['0'])]), Tree('X_5=2', [Tree('X_7=0', ['1']), Tree('X_7=1', ['1']), Tree('X_7=2', ['1']), Tree('X_7=3', ['1']), Tree('X_7=4', ['0'])]), Tree('X_5=3', [Tree('X_7=0', ['1']), Tree('X_7=1', ['1']), Tree('X_7=2', ['1']), Tree('X_7=3', ['1']), Tree('X_7=4', ['0'])]), Tree('X_5=4', [Tree('X_14=0', ['0']), Tree('X_14=1', ['0']), Tree('X_14=2', ['0']), Tree('X_14=3', ['0']), Tree('X_14=4', ['0'])])])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.plot_tree(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [10, 8, 5, 6, 5, 7, 10, 10, 10, 4, 8, 9, 5, 7, 10, 6, 2, 3, 2, 8]\n",
    "d = len(categories)\n",
    "labels = [0, 1]\n",
    "actions = range(d + len(labels))\n",
    "\n",
    "# Feature variables are independent and uniform\n",
    "generator = dict([(i, np.full(categories[i], 1/categories[i])) for i in range(len(categories))])\n",
    "\n",
    "# Each query action costs -1\n",
    "rewards_queries = dict([(i, -.5) for i in range(len(categories))])\n",
    "\n",
    "encoder = Encoder(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(generator, rewards_queries, encoder, r_plus=5, r_minus=-10, split=9)\n",
    "agent = Agent(categories=categories, min_queries=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0\n",
      "Episode : 1000\n",
      "Episode : 2000\n",
      "Episode : 3000\n",
      "Episode : 4000\n",
      "Episode : 5000\n",
      "Episode : 6000\n",
      "Episode : 7000\n",
      "Episode : 8000\n",
      "Episode : 9000\n",
      "Episode : 10000\n",
      "Episode : 11000\n",
      "Episode : 12000\n",
      "Episode : 13000\n",
      "Episode : 14000\n",
      "Episode : 15000\n",
      "Episode : 16000\n",
      "Episode : 17000\n",
      "Episode : 18000\n",
      "Episode : 19000\n",
      "Episode : 20000\n",
      "Episode : 21000\n",
      "Episode : 22000\n",
      "Episode : 23000\n",
      "Episode : 24000\n",
      "Episode : 25000\n",
      "Episode : 26000\n",
      "Episode : 27000\n",
      "Episode : 28000\n",
      "Episode : 29000\n",
      "Episode : 30000\n",
      "Episode : 31000\n",
      "Episode : 32000\n",
      "Episode : 33000\n",
      "Episode : 34000\n",
      "Episode : 35000\n",
      "Episode : 36000\n",
      "Episode : 37000\n",
      "Episode : 38000\n",
      "Episode : 39000\n",
      "Episode : 40000\n",
      "Episode : 41000\n",
      "Episode : 42000\n",
      "Episode : 43000\n",
      "Episode : 44000\n",
      "Episode : 45000\n",
      "Episode : 46000\n",
      "Episode : 47000\n",
      "Episode : 48000\n",
      "Episode : 49000\n",
      "Episode : 50000\n",
      "Episode : 51000\n",
      "Episode : 52000\n",
      "Episode : 53000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7648df4f1b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'runs_new_exp_reinforce/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-8d6d066110d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, n_train, lr, log_dir, n_prints, max_step, lambd, clip_grad)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Losses/Actor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Losses/Critic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Losses/Entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Losses/Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mscalar_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         self._get_file_writer().add_summary(\n\u001b[0;32m--> 342\u001b[0;31m             scalar(tag, scalar_value), global_step, walltime)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_scalar_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/tensorboard/summary.py\u001b[0m in \u001b[0;36mscalar\u001b[0;34m(name, scalar, collections)\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \"\"\"\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scalar should be 0D'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/tensorboard/_convert_np.py\u001b[0m in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_prepare_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     raise NotImplementedError(\n\u001b[1;32m     30\u001b[0m         'Got {}, but numpy array, torch tensor, or caffe2 blob name are expected.'.format(type(x)))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/tensorboard/_convert_np.py\u001b[0m in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(env, n_train=500000, lr=3e-4, lambd=1, max_step=30, log_dir='runs_new_exp_reinforce/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test(env, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred, labels_true = [], []\n",
    "for i in range(10000):\n",
    "    data_point = env.generate()\n",
    "    labels_pred.append(agent.predict(env, data_point))\n",
    "    labels_true.append(env.concept(data_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true ones      : 5841\n",
      "Number of predicted ones : 5841\n"
     ]
    }
   ],
   "source": [
    "print('Number of true ones      : %d' %np.sum(labels_true))\n",
    "print('Number of predicted ones : %d' %np.sum(labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for i in range(1000):\n",
    "    data_point = env.generate()\n",
    "    env.reset(data_point)\n",
    "    state = env.state\n",
    "    depth = 0\n",
    "    while not env.done:\n",
    "        action = agent.action_greedy(state)\n",
    "        _, state, _ = env.step(action)\n",
    "        depth += 1\n",
    "        \n",
    "    depths.append(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa3c0a75890>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(depths, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# agent.plot_tree(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_weights('reinforce_new_exp_reinforce.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
