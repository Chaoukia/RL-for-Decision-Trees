{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from nltk import Tree\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class represeting the one-hot encoder of the states.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Encoder.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        d          : Int, the input dimension.\n",
    "        dim        : Int, the one-hot encoded representation dimension\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.categories = categories\n",
    "        self.d = len(categories)\n",
    "        self.dim = np.sum(categories) + self.d\n",
    "        \n",
    "    def transform(self, state_values):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the vector state representation with dummies.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        state_values : List of length d where the ith entry is either NaN or the the feature value.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        state_one_hot : 2D np.array representing the one-hot encoded state.\n",
    "        \"\"\"\n",
    "    \n",
    "        s = 0\n",
    "        state_one_hot = np.zeros((1, self.dim), dtype = np.float32)\n",
    "        for i, value in enumerate(state_values):\n",
    "            if np.isnan(value):\n",
    "                state_one_hot[0, self.categories[i] + s] = 1\n",
    "\n",
    "            else:\n",
    "                state_one_hot[0, value + s] = 1\n",
    "\n",
    "            s += self.categories[i]+1\n",
    "\n",
    "        return state_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing a state, it also serves as Node representation\n",
    "    for our Breadth First Search\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, values, encoder, categories=[4, 3, 3, 3, 2, 4]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class State.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        values     : List of length d (Input dimension):\n",
    "                         - values[i] = NaN if i is an unobsorved feature.\n",
    "                         - values[i] = value of feature i if it is observed.\n",
    "        encocer    : sklearn.preprocessing._encoders.OneHotEncoder object.\n",
    "        categories : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        observed   : List containing the observed features at this state.\n",
    "        unobserved : List containing the unobserved features at this state.\n",
    "        empty      : Boolean, whether it is the empty state or not.\n",
    "        complete   : Boolean, whether all features are observed or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        d = len(values)\n",
    "        values_nans = np.isnan(values)\n",
    "        self.encoder = encoder  # One-hot encoder, used in the approximate RL framework for state representation.\n",
    "        self.values = values\n",
    "        self.values_encoded = self.encode()\n",
    "        self.categories = categories\n",
    "        self.observed = np.arange(d)[np.invert(values_nans)]\n",
    "        self.unobserved = np.arange(d)[values_nans]  # These are also the allowed query actions at this state\n",
    "        self.empty = (len(self.observed) == 0)\n",
    "        self.complete = (len(self.unobserved) == 0)\n",
    "        \n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Encode the state with dummy variables. To be used when a one-hot encoder is defined.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        np.array of shape (1, #one_hot_representation_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.encoder.transform(self.values)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        String representation of the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = '| '\n",
    "        for x in self.values:\n",
    "            s += str(x) + ' | '\n",
    "            \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=25, out=8):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        Constructor of Deep Q-network class.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        input_size : Int, the one-hot encoding representation dimension.\n",
    "        out        : Int, output dimension, equal to the number of possible actions.\n",
    "        fc_1       : nn.Linear, first fully connected layer.\n",
    "        fc_2       : nn.Linear, second fully connected layer.\n",
    "        output     : nn.Linear, output fully connected layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_size, 32)\n",
    "        self.fc_2 = nn.Linear(32, 32)\n",
    "        self.output = nn.Linear(32, out)\n",
    "        \n",
    "        # Initialization\n",
    "        nn.init.constant_(self.fc_1.weight, 0)\n",
    "        nn.init.constant_(self.fc_1.bias, 0)\n",
    "        nn.init.constant_(self.fc_2.weight, 0)\n",
    "        nn.init.constant_(self.fc_2.bias, 0)\n",
    "        nn.init.constant_(self.output.weight, 0)\n",
    "        nn.init.constant_(self.output.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        The forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        x : torch.tensor of dimension (batch_size, input_size)\n",
    "        \n",
    "        Returns\n",
    "        ---------------\n",
    "        torch.tensor of dimension (batch_size, out)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class representing the environment, it can generate data points that start each episode,\n",
    "    keep track of the current state, return the reward of an action taken at the current state,\n",
    "    and transition to the next corresponding state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, generator, rewards_queries, encoder, r_plus=5, r_minus=-5, split=3):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class Environment.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        generator       : Dict, - keys   : Feature variables.\n",
    "                                - values : List of probability masses of each category of the corresponding feature.\n",
    "        rewards_queries : Dict, - keys   : Feature variables.\n",
    "                                - values : Reward of querying the value of the corresponding feature.\n",
    "        encocer         : sklearn.preprocessing._encoders.OneHotEncoder object.\n",
    "        r_plus          : Int, reward of a correct report (default=5).\n",
    "        r_minus         : Int, reward of an incorrect report (default=-5).\n",
    "        split           : Int, the split point we use to define our concept.\n",
    "        d               : Int, the number of feature variables.\n",
    "        data_point      : List of length d, the data point starting the episode.\n",
    "        label           : Boolean, the true label of data_point.\n",
    "        state           : Object of class State, the current state.\n",
    "        done            : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.generator = generator\n",
    "        self.categories = [len(v) for v in self.generator.values()]\n",
    "        self.d = len(self.categories)\n",
    "        self.rewards_queries = rewards_queries\n",
    "        self.encoder = encoder\n",
    "        self.r_plus = r_plus\n",
    "        self.r_minus = r_minus\n",
    "        self.split = split\n",
    "        \n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Generate a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        List with the values of each feature, it represents the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        return [np.random.choice(self.categories[i], p=self.generator[i]) for i in range(self.d)]\n",
    "    \n",
    "    def concept(self, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Define the concept labeling the data points. we can define it as a decision tree for example.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        Int in {0, 1}, the label of the data point.\n",
    "        \"\"\"\n",
    "        \n",
    "        label = True\n",
    "        i = 0\n",
    "        while label and i <= self.d-1:\n",
    "            if data_point[i] >= self.split:\n",
    "                label = False\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def reset(self, data_point=None):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Reset the environment to start a new episode. If data_point is specified, start the episode from it,\n",
    "        otherwise generate it.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        data_point : List of length d, the data point to label (default=None).\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_point = self.generate() if data_point is None else data_point\n",
    "        self.label = self.concept(self.data_point)\n",
    "        self.state = State([np.NaN for i in range(self.d)], self.encoder, categories=self.categories)\n",
    "        self.done = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Interract with the environment through an action taken at the current state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}, \n",
    "                 - 0, ..., d-1 represent query actions.\n",
    "                 - d, d+1 represent report actions.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        reward     : Int, the reward of taking this action at the current state.\n",
    "        next_state : Object of class State, the next state.\n",
    "        done       : Boolean, whether the episode is finished or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Treating query actions.\n",
    "        if action <= self.d-1:\n",
    "            # If it is an allowed query action.\n",
    "            if np.isnan(self.state.values[action]):\n",
    "                reward = self.rewards_queries[action]\n",
    "                values = self.state.values\n",
    "                values[action] = self.data_point[action] # Reveal the value of the queried feature in the data point.\n",
    "                self.state = State(values, self.encoder, self.categories)\n",
    "                \n",
    "            # If this query action is not allowed.\n",
    "            else:\n",
    "                print('unallowed')\n",
    "            \n",
    "        # Treating report actions.\n",
    "        else:\n",
    "            reward = self.r_plus if (action%self.d) == self.label else self.r_minus\n",
    "            self.done = True\n",
    "            \n",
    "        return reward, self.state, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=25, out=8):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        Constructor of Deep Q-network class.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        input_size : Int, the one-hot encoding representation dimension.\n",
    "        out        : Int, output dimension, equal to the number of possible actions.\n",
    "        fc_1       : nn.Linear, first fully connected layer.\n",
    "        fc_2       : nn.Linear, second fully connected layer.\n",
    "        output     : nn.Linear, output fully connected layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_size, 64)\n",
    "        self.fc_2 = nn.Linear(64, 32)\n",
    "        self.fc_3 = nn.Linear(32, 32)\n",
    "        self.output = nn.Linear(32, out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        The forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        x : torch.tensor of dimension (batch_size, input_size)\n",
    "        \n",
    "        Returns\n",
    "        ---------------\n",
    "        torch.tensor of dimension (batch_size, out)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        x = F.relu(self.fc_3(x))\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    --------------\n",
    "    Class describing a DQN agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=.9, categories=[4, 3, 3, 3, 2, 4], labels=[0, 1]):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Constructor of class AgentDQN.\n",
    "        \n",
    "        Parameters & Attributes\n",
    "        --------------\n",
    "        gamma           : Float in ]0, 1[, the discount factor (default=0.9).\n",
    "        categories      : List of length d where categories[i] is the number of categories feature i can take.\n",
    "        labels          : List of the possible labels.\n",
    "        d               : Int, the number of feature variables.\n",
    "        b               : Int, the number of class labels.\n",
    "        actions         : List of all actions.\n",
    "        actions_queries : List of query actions.\n",
    "        actions_report  : List of report actions.\n",
    "\n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.categories = categories\n",
    "        self.labels = labels\n",
    "        self.d = len(categories)\n",
    "        self.b = len(labels)\n",
    "        self.actions = range(self.d + len(labels))\n",
    "        self.actions_queries = range(d)\n",
    "        self.actions_report = [self.d + label for label in labels]\n",
    "        self.policy = Policy(input_size=np.sum(categories)+self.d, out=self.d+self.b)\n",
    "#         self.q_network = DQNetwork(input_size=np.sum(categories)+self.d, out=self.d+self.b)\n",
    "#         self.q_network.load_state_dict(torch.load('ddqn_weights.pth'))\n",
    "        \n",
    "        \n",
    "    def actions_probas(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Calculate the probabilities of the allowed actions at a state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state   : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        actions_probas  : torch.tensor of size #Allowed_actions, with the allowed actions probabilities.\n",
    "        actions_allowed : List of the allowed actions.\n",
    "        \"\"\"\n",
    "        \n",
    "        actions_output = self.policy(torch.from_numpy(state.values_encoded))\n",
    "        actions_allowed = list(state.unobserved) + [self.d+i for i in range(self.b)]\n",
    "        actions_probas = F.softmax(actions_output[0, actions_allowed], dim=0)\n",
    "        return actions_probas, actions_allowed\n",
    "        \n",
    "    def action(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose an action at a state by sampling from the current stochastic policy.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state  : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        action, action_proba : Int in {0, ..., d-1, d, d+1}, Float in [0, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        actions_probas, actions_allowed = self.actions_probas(state)\n",
    "        index = np.random.choice(len(actions_allowed), p=actions_probas.detach().numpy())\n",
    "        action, action_proba = actions_allowed[index], actions_probas[index]\n",
    "        return action, action_proba, index\n",
    "    \n",
    "    def action_greedy(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Choose the action maximizing the stochastic policy at the state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state  : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        action : Int in {0, ..., d-1, d, d+1}\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions_probas, actions_allowed = self.actions_probas(state)\n",
    "            return actions_allowed[torch.argmax(actions_probas).item()]\n",
    "        \n",
    "        \n",
    "    def train(self, env, n_train=1000, lr=1e-4, log_dir='runs_pg/', n_prints=1000):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Explore the environment and train the agent.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env      : Object of class Environment.\n",
    "        n_train  : Int, number of training episodes.\n",
    "        lr       : Float, the learning rate.\n",
    "        log_dir  : String, path of the folder where tensorboard events are stored.\n",
    "        n_prints : Int, the number of iterations between two consecutive prints.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "        optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        variables = {'return' : []}\n",
    "        for episode in range(n_train):\n",
    "            env.reset()\n",
    "            state = deepcopy(env.state)\n",
    "            states, actions_indices, actions, rewards_gamma = [], [], [], []\n",
    "            step = 0\n",
    "            while not env.done:\n",
    "                action, action_proba, action_index = self.action(state)\n",
    "                reward, next_state, _ = env.step(action)\n",
    "                states.append(state)\n",
    "                actions_indices.append(action_index)\n",
    "                actions.append(action)\n",
    "                rewards_gamma.append((self.gamma**step)*torch.tensor(reward, dtype=torch.float32, requires_grad=False).reshape(-1, 1))\n",
    "                state = deepcopy(next_state)\n",
    "                variables['return'].append(reward)\n",
    "                step += 1\n",
    "                \n",
    "            # Learning phase.\n",
    "            rewards_gamma_episode = torch.cat(rewards_gamma).flip(0, 1).cumsum(0).flip(0, 1)\n",
    "            for t in range(step):\n",
    "                state, action_index, action = states[t], actions_indices[t], actions[t]\n",
    "                actions_probas, actions_allowed = self.actions_probas(state)\n",
    "                ln_action_proba = torch.log(actions_probas[action_index])\n",
    "                loss = -((self.gamma**(-t))*rewards_gamma_episode[t, 0])*ln_action_proba\n",
    "#                 loss = -((self.gamma**(-t))*rewards_gamma_episode[t, 0] - self.q_network(torch.from_numpy(state.values_encoded)).mean().item())*ln_action_proba\n",
    "#                 loss = -self.q_network(torch.from_numpy(state.values_encoded))[0, action]*ln_action_proba\n",
    "                optimizer.zero_grad()\n",
    "                if t == step-1:\n",
    "                    loss.backward()\n",
    "                    \n",
    "                else:\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    \n",
    "                optimizer.step()\n",
    "            \n",
    "            writer.add_scalar('Return', np.sum(variables['return']), episode)\n",
    "            variables['return'] = []\n",
    "            if episode%n_prints == 0:\n",
    "                print('Episode : %d' %(episode))\n",
    "                        \n",
    "        writer.close()\n",
    "        \n",
    "    def predict(self, env, data_point):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Predict the label of a data point.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        env.reset(data_point)\n",
    "        state = env.state\n",
    "        while not env.done:\n",
    "            action = agent.action_greedy(state)\n",
    "            env.step(action)\n",
    "            state = env.state\n",
    "        \n",
    "        return action%self.d\n",
    "        \n",
    "    def test(self, env, n_test=1000):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Test the agent on n_test data points generated by env.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        env      : Object of class Environment.\n",
    "        n_test   : Int, number of data points to test the agent on.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        accuracy : FLoat in [0, 1], the accuracy of the agent on this test.\n",
    "        \"\"\"\n",
    "        \n",
    "        valids = 0\n",
    "        for i in range(n_test):\n",
    "            data_point = env.generate()\n",
    "            env.reset(data_point)\n",
    "            label_pred, label_true = self.predict(env, data_point), env.label\n",
    "            valids += (label_pred==label_true)\n",
    "            \n",
    "        return valids/n_test\n",
    "    \n",
    "    def save_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Save the agents q-network weights.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Load the weights of a q-network.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        path: String, path to a .pth file containing the weights of a q-network.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        \"\"\"\n",
    "        \n",
    "        self.policy.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def children(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Give the possible outcomes of taking the greedy policy at the considered state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        children : Set of objects of class State.\n",
    "        action   : Int, action taken at state with the agent policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        children = []\n",
    "        action = self.action_greedy(state)\n",
    "        if action >= self.d: return children, action\n",
    "        for category in range(self.categories[action]):\n",
    "            values = state.values.copy()\n",
    "            values[action] = category\n",
    "            children.append(State(values, state.encoder, self.categories))\n",
    "\n",
    "        return children, action\n",
    "    \n",
    "    def build_string_state(self, state):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision (with parentheses) starting from state.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        state : Object of class State.\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        l, action = self.children(state)\n",
    "        if action >= self.d: return str(self.action_greedy(state)%self.d) + ''\n",
    "        string = ''\n",
    "        for child in l:\n",
    "            string += '(X_' + str(action) + '=' + str(child.values[action]) + ' ' + self.build_string_state(child) + ') '\n",
    "\n",
    "        return string\n",
    "    \n",
    "    def build_string(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Build string representation of the agent decision.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        string : String representation of a tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        return '( ' + self.build_string_state(State([np.NaN for i in range(self.d)], encoder, self.categories)) + ')'\n",
    "    \n",
    "    def plot_tree(self, encoder):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        --------------\n",
    "        Plot the agent's decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        \n",
    "        Returns\n",
    "        --------------\n",
    "        nltk tree object, helpful to visualize the agent's decision tree policy.\n",
    "        \"\"\"\n",
    "\n",
    "        return Tree.fromstring(self.build_string(encoder))\n",
    "        \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [3, 5, 3, 3, 3, 5, 3, 5, 3, 3, 4, 3, 3, 3, 5]\n",
    "labels = [0, 1]\n",
    "d, b = len(categories), len(labels)\n",
    "actions = range(d + len(labels))\n",
    "\n",
    "# Feature variables are independent and uniform\n",
    "generator = dict([(i, np.full(categories[i], 1/categories[i])) for i in range(len(categories))])\n",
    "\n",
    "# Each query action costs -1\n",
    "rewards_queries = dict([(i, -1) for i in range(len(categories))])\n",
    "\n",
    "# Define ethe encoder\n",
    "encoder = Encoder(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(generator, rewards_queries, encoder, r_plus=5, r_minus=-5, split=4)\n",
    "agent = Agent(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = []\n",
    "# for i in range(10000):\n",
    "#     data_point = env.generate()\n",
    "#     l.append(env.concept(data_point))\n",
    "\n",
    "# print('Fraction of 1 labeled points : %.5f' %(np.sum(l)/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0\n",
      "Episode : 1000\n",
      "Episode : 2000\n",
      "Episode : 3000\n",
      "Episode : 4000\n",
      "Episode : 5000\n",
      "Episode : 6000\n",
      "Episode : 7000\n",
      "Episode : 8000\n",
      "Episode : 9000\n",
      "Episode : 10000\n",
      "Episode : 11000\n",
      "Episode : 12000\n",
      "Episode : 13000\n",
      "Episode : 14000\n",
      "Episode : 15000\n",
      "Episode : 16000\n",
      "Episode : 17000\n",
      "Episode : 18000\n",
      "Episode : 19000\n",
      "Episode : 20000\n",
      "Episode : 21000\n",
      "Episode : 22000\n",
      "Episode : 23000\n",
      "Episode : 24000\n",
      "Episode : 25000\n",
      "Episode : 26000\n",
      "Episode : 27000\n",
      "Episode : 28000\n",
      "Episode : 29000\n",
      "Episode : 30000\n",
      "Episode : 31000\n",
      "Episode : 32000\n",
      "Episode : 33000\n",
      "Episode : 34000\n",
      "Episode : 35000\n",
      "Episode : 36000\n",
      "Episode : 37000\n",
      "Episode : 38000\n",
      "Episode : 39000\n",
      "Episode : 40000\n",
      "Episode : 41000\n",
      "Episode : 42000\n",
      "Episode : 43000\n",
      "Episode : 44000\n",
      "Episode : 45000\n",
      "Episode : 46000\n",
      "Episode : 47000\n",
      "Episode : 48000\n",
      "Episode : 49000\n",
      "Episode : 50000\n",
      "Episode : 51000\n",
      "Episode : 52000\n",
      "Episode : 53000\n",
      "Episode : 54000\n",
      "Episode : 55000\n",
      "Episode : 56000\n",
      "Episode : 57000\n",
      "Episode : 58000\n",
      "Episode : 59000\n",
      "Episode : 60000\n",
      "Episode : 61000\n",
      "Episode : 62000\n",
      "Episode : 63000\n",
      "Episode : 64000\n",
      "Episode : 65000\n",
      "Episode : 66000\n",
      "Episode : 67000\n",
      "Episode : 68000\n",
      "Episode : 69000\n",
      "Episode : 70000\n",
      "Episode : 71000\n",
      "Episode : 72000\n",
      "Episode : 73000\n",
      "Episode : 74000\n",
      "Episode : 75000\n",
      "Episode : 76000\n",
      "Episode : 77000\n",
      "Episode : 78000\n",
      "Episode : 79000\n",
      "Episode : 80000\n",
      "Episode : 81000\n",
      "Episode : 82000\n",
      "Episode : 83000\n",
      "Episode : 84000\n",
      "Episode : 85000\n",
      "Episode : 86000\n",
      "Episode : 87000\n",
      "Episode : 88000\n",
      "Episode : 89000\n",
      "Episode : 90000\n",
      "Episode : 91000\n",
      "Episode : 92000\n",
      "Episode : 93000\n",
      "Episode : 94000\n",
      "Episode : 95000\n",
      "Episode : 96000\n",
      "Episode : 97000\n",
      "Episode : 98000\n",
      "Episode : 99000\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, n_train=100000, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.load_weights('pg_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5887"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test(env, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred, labels_true = [], []\n",
    "for i in range(10000):\n",
    "    data_point = env.generate()\n",
    "    labels_pred.append(agent.predict(env, data_point))\n",
    "    labels_true.append(env.concept(data_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true ones      : 4205\n",
      "Number of predicted ones : 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of true ones      : %d' %np.sum(labels_true))\n",
    "print('Number of predicted ones : %d' %np.sum(labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for i in range(1000):\n",
    "    data_point = env.generate()\n",
    "    env.reset(data_point)\n",
    "    state = env.state\n",
    "    depth = 0\n",
    "    while not env.done:\n",
    "#         action, _, _ = agent.action_greedy(state)\n",
    "        action = agent.action_greedy(state)\n",
    "        _, state, _ = env.step(action)\n",
    "        depth += 1\n",
    "        \n",
    "    depths.append(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f324dfe9210>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(depths, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5374, -1.2231, -1.2651, -1.4924, -1.8946, -0.6104, -2.6298, -2.4570,\n",
       "         -2.8781, -1.0633, -1.4204, -1.7187, -1.0419, -1.2124, -2.6216,  6.9567,\n",
       "         -2.7422]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy(torch.from_numpy(State([np.NaN for i in range(d)], encoder).values_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 0, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([torch.tensor(1), torch.tensor(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
